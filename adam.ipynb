{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import openpyxl\n",
    "'''train = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")'''\n",
    "'''\n",
    "train = np.array([\n",
    "    [0,1,1,1,3],\n",
    "    [1,1,1,1,4],\n",
    "    [2,1,1,1,5],\n",
    "    [3,1,1,1,6]\n",
    "]\n",
    ")'''\n",
    "train = pd.read_csv('train.csv').as_matrix()#read train.csv file\n",
    "#print(type(train))\n",
    "#xtrain=train[:,1:4]\n",
    "#xtrain = xtrain/5\n",
    "#true_label=train[:,4:5]\n",
    "#print((xtrain),(true_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_into_prob(x,max_val):\n",
    "    len=x.size\n",
    "    y_list=[]\n",
    "    for i in range(len):\n",
    "        temp=[]\n",
    "        for j in range(max_val):\n",
    "            if(x[i]==j):\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "            #print(temp)\n",
    "        y_list.append(temp)\n",
    "    return y_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55945675 -0.55945675  2.06254185 ...  2.0522595  -0.55945675\n",
      "  -0.55945675]\n",
      " [-0.5380259  -0.5380259  -0.5380259  ... -0.5380259  -0.5380259\n",
      "  -0.5380259 ]\n",
      " [-0.6486501  -0.6486501  -0.6486501  ... -0.6486501  -0.6486501\n",
      "  -0.6486501 ]\n",
      " ...\n",
      " [-0.64973918 -0.64973918 -0.64973918 ... -0.64973918 -0.64973918\n",
      "  -0.64973918]\n",
      " [-0.59745866 -0.59745866 -0.59745866 ... -0.59745866 -0.59745866\n",
      "  -0.59745866]\n",
      " [-0.62629522 -0.62629522 -0.62629522 ... -0.62629522 -0.62629522\n",
      "  -0.62629522]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "xtrain=train[:,1:785]\n",
    "\n",
    "xmean = np.mean(xtrain , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "xtrain = xtrain - means_expanded\n",
    "\n",
    "xstd = np.std(xtrain , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "xtrain = xtrain*1.0/std_expanded \n",
    "\n",
    "true_label = train[:,785:]\n",
    "y_true=np.array(encode_into_prob(true_label,10))\n",
    "y_true=y_true.transpose()\n",
    "\n",
    "#print(type(y_true))\n",
    "#print(y_true.transpose())\n",
    "print(xtrain)\n",
    "print((y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(batch_train,hidden_layers):\n",
    "     \n",
    "    global weight_matrix_list\n",
    "    global bias_list\n",
    "    global preactivation_list \n",
    "    preactivation_list=[]\n",
    "    global activation_list\n",
    "    activation_list=[]\n",
    "    global y_hat\n",
    "    y_hat=[]\n",
    "    a_new=[]\n",
    "    a_last_layer=[]\n",
    "    nos_data = len(batch_train)\n",
    "    #print(nos_data)\n",
    "    h=batch_train.transpose()\n",
    "    preactivation_list.append(h)\n",
    "    activation_list.append(h)\n",
    "    #print(h)\n",
    "    \n",
    "#     print(\"y_hat in feed\\n\",y_hat )\n",
    "    \n",
    "    for j in range(hidden_layers):\n",
    "        \n",
    "#         print(\".............................\")\n",
    "#         print(\"activation for layer \", j)\n",
    "#         print(activation_list[j] , activation_list[j].shape)\n",
    "#         print(\"weight\",j+1)\n",
    "#         print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#         print(\"bias \",j+1)\n",
    "#         print(bias_list[j],bias_list[j].shape)\n",
    "        \n",
    "        a_new = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "        preactivation_list.append(a_new)\n",
    "        \n",
    "#         print(\"preactivation for layer\",j+1)\n",
    "#         print(preactivation_list[j+1], preactivation_list[j+1].shape)\n",
    "        \n",
    "        h = 1.0/(1.0+np.exp(-1*preactivation_list[j+1]))\n",
    "        activation_list.append(h)\n",
    "\n",
    "    \n",
    "    j=hidden_layers\n",
    "#     print(\".............................\")\n",
    "#     print(\"activation for layer \", j)\n",
    "#     print(activation_list[j] , activation_list[j].shape)\n",
    "#     print(\"weight\",j+1)\n",
    "#     print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#     print(\"bias \",j+1)\n",
    "#     print(bias_list[j],bias_list[j].shape)\n",
    "    \n",
    "    a_last_layer = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "    preactivation_list.append(a_last_layer)\n",
    "    \n",
    "#     print(\"last activation\")\n",
    "#     print(a_last_layer,a_last_layer.shape)\n",
    "\n",
    "    y_hat = np.array(output(a_last_layer,batch_train))\n",
    "    y_hat = y_hat.transpose()\n",
    "    \n",
    "#     print(\"y_hat in feedforward\")\n",
    "#     print(y_hat, y_hat.shape)\n",
    "    \n",
    "    \n",
    "def output(a_last_layer,batch_train):\n",
    "    y_hat_initial=[]\n",
    "    y_hat_local=[]\n",
    "    nos_data=len(batch_train)\n",
    "    for i in range(nos_data):\n",
    "        y_hat_initial.append(softmax(a_last_layer[:,i:i+1].transpose()))\n",
    "\n",
    "    length = len(y_hat_initial)\n",
    "    for i in range(length):\n",
    "        y_hat_local.append(y_hat_initial[i][0])\n",
    "    \n",
    "#     print(\"y_hat_local\", y_hat_local)\n",
    "#     print(\"y_hat_initial\",y_hat_initial)\n",
    "    return y_hat_local\n",
    "\n",
    "def softmax(row_vector):\n",
    "    sum_all=0\n",
    "    row_vector = np.exp(row_vector)\n",
    "    sum_all = np.sum(row_vector , axis=1)         #print(\"row_vector\", row_vector)\n",
    "    row_vector = (row_vector*1.0/sum_all)         #print(\"sum_all\",sum_all)\n",
    "    return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(y_true_batch):\n",
    "    \n",
    "#     print(\"weigt in backpropogation\", weight_matrix_list)\n",
    "#     print(\"...........y_true_batch..........\\n\",y_true_batch)\n",
    "    \n",
    "    global grad_a_k_list\n",
    "    grad_a_k_list=[]\n",
    "    \n",
    "    global grad_w_k_list\n",
    "    grad_w_k_list=[]\n",
    "    \n",
    "    global grad_h_k_list\n",
    "    grad_h_k_list=[]\n",
    "    \n",
    "    global grad_b_k_list\n",
    "    grad_b_k_list=[]\n",
    "    \n",
    "    global grad_b\n",
    "    grad_b=[]\n",
    "    \n",
    "    global grad_g_dash_a_k_list\n",
    "    grad_g_dash_a_k_list = []\n",
    "    \n",
    "    global grad_a_last\n",
    "    grad_a_last=[]\n",
    "    \n",
    "    k = num_hidden+1\n",
    "    \n",
    "#     print(\"--------------------------in Backpropagation---------------------\")\n",
    "    \n",
    "    grad_g_dash_a_k_list = g_dash(preactivation_list)\n",
    "    \n",
    "#     print(\"preactivation_list....................\")\n",
    "#     print(\"Nos. of matrices in preactivation_list is\", len(preactivation_list))\n",
    "#     print(preactivation_list)\n",
    "#     print(\"g_dash..................\")\n",
    "#     print(\"Nos. of matrices in g_dash is \", len(grad_g_dash_a_k_list))\n",
    "#     print(grad_g_dash_a_k_list)\n",
    "    \n",
    "    grad_a_last = -np.subtract(y_true_batch,y_hat)\n",
    "    \n",
    "#     print(\"y_true_batch \\n\", y_true_batch)\n",
    "#     print(\"y_hat \\n\",y_hat)\n",
    "    \n",
    "    grad_a_k_list.insert(0,grad_a_last)\n",
    "    \n",
    "#     print(\"grad_a_last_layer\", k)\n",
    "#     print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "        \n",
    "    while(k>0):\n",
    "        \n",
    "#         print(\".................grad_compute at layer\",k)\n",
    "        \n",
    "        grad_w_k_list.insert(0,np.matmul(grad_a_k_list[0],activation_list[k-1].transpose()))\n",
    "        \n",
    "#         print(\"grad_a\", k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print('preactivation  a_',k-1)\n",
    "#         print(preactivation_list[k-1],preactivation_list[k-1].shape)\n",
    "#         print(\"grad_w_\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "        \n",
    "        grad_b_k_list.insert(0,grad_a_k_list[0])\n",
    "        \n",
    "#         print(\"grad_b\",k)\n",
    "#         print(grad_b_k_list[0],grad_b_k_list[0].shape)\n",
    "        \n",
    "        grad_h_k_list.insert(0,np.matmul(weight_matrix_list[k-1],grad_a_k_list[0]))\n",
    "        \n",
    "#         print(\"grad_w\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "#         print(\"grad_a\",k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print(\"grad_h\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "        \n",
    "        grad_a_k_list.insert(0,np.multiply(grad_h_k_list[0],grad_g_dash_a_k_list[k-1]))\n",
    "        \n",
    "#         print(\"grad_h_k_list[0]\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "#         print(\"grad_g_dash_a_k_list[k-1]\",k-1)\n",
    "#         print(grad_g_dash_a_k_list[k-1],grad_g_dash_a_k_list[k-1].shape)\n",
    "#         print(\"grad_a\",k-1)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "        k=k-1\n",
    "#         print(\"grad_a_ik_list\",grad_a_k_list[0])\n",
    "    \n",
    "    sum_all_grad_b_of_data(grad_b_k_list)\n",
    "    \n",
    "\n",
    "def sum_all_grad_b_of_data(grad_b_k_list):\n",
    "    length=len(grad_b_k_list)\n",
    "    i=0\n",
    "    while(i<length):\n",
    "        row=[]\n",
    "        row = np.sum(grad_b_k_list[i],1)\n",
    "        j = len(row)\n",
    "        #print(row.shape)\n",
    "        row = row[:, np.newaxis]\n",
    "        #print(row.shape , row)\n",
    "        grad_b.append(row)\n",
    "        i+=1\n",
    "    #print(\"grad_b\",grad_b)\n",
    "\n",
    "def g_dash(preactivation_list):\n",
    "    length=len(preactivation_list)\n",
    "    a_modified = preactivation_list[0:length]\n",
    "    g_dash_a_k_list=[]\n",
    "    for i in range(length):\n",
    "        a=(1.0/(1+np.exp(-a_modified[i])))*(1-(1.0/(1+np.exp(-a_modified[i]))))\n",
    "        g_dash_a_k_list.append(a)\n",
    "    return g_dash_a_k_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_yhat_to_classes(y_hat_local):\n",
    "    col = len(y_hat_local[0])\n",
    "    max_val=0\n",
    "    for j in range(col):\n",
    "        max_val = y_hat_local[0][j]\n",
    "        index=0\n",
    "        for i in range(9):\n",
    "            if(y_hat_local[i+1][j] > max_val):\n",
    "                y_hat_local[index][j]=0\n",
    "                index = i+1\n",
    "                max_val=y_hat[i+1][j]\n",
    "            else :\n",
    "                y_hat_local[i+1][j]=0\n",
    "        y_hat_local[index][j]=1\n",
    "    return y_hat_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 10]\n",
      "2\n",
      "[array([[0.00140621, 0.00970851, 0.00100002, ..., 0.00474145, 0.00426977,\n",
      "        0.0046401 ],\n",
      "       [0.00824713, 0.00351868, 0.00859146, ..., 0.00435939, 0.00077917,\n",
      "        0.00614494],\n",
      "       [0.00994193, 0.0077316 , 0.00409722, ..., 0.00948514, 0.00735271,\n",
      "        0.00888722],\n",
      "       ...,\n",
      "       [0.00531261, 0.00191074, 0.00964108, ..., 0.00280564, 0.0053278 ,\n",
      "        0.00022512],\n",
      "       [0.00295578, 0.00769845, 0.00605919, ..., 0.00982787, 0.00670437,\n",
      "        0.00929958],\n",
      "       [0.00471208, 0.00848609, 0.00222295, ..., 0.00265812, 0.00867386,\n",
      "        0.00645128]])]\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n"
     ]
    }
   ],
   "source": [
    "#ip_neurons = 3 #input layer\n",
    "ip_neurons = 784\n",
    "num_hidden = 0\n",
    "encoding_bits=10\n",
    "sizes = []\n",
    "sizes.insert(0,ip_neurons)\n",
    "sizes.append(encoding_bits)\n",
    "print(sizes)\n",
    "print(len(sizes))\n",
    "\n",
    "weight_matrix_list = []\n",
    "\n",
    "bias_list =[]\n",
    "y_hat= []\n",
    "\n",
    "for i in range(int(num_hidden)+1):\n",
    "    weight_matrix_list.append(np.random.rand(sizes[i],sizes[i+1])/np.sqrt(sizes[i]))\n",
    "    bias_list.append(0*np.random.rand(sizes[i+1],1))\n",
    "print(weight_matrix_list)\n",
    "#print(len(weight_matrix_list))\n",
    "\n",
    "print(bias_list)\n",
    "#print(hlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "    \n",
    "    m_w = [] \n",
    "    m_b = []\n",
    "    for i in range(int(num_hidden)+1):\n",
    "        m_w.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "        m_b.append(0*np.random.rand(sizes[i+1],1))\n",
    "    \n",
    "    v_w = [] \n",
    "    v_b =[]\n",
    "    for i in range(int(num_hidden)+1):\n",
    "        v_w.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "        v_b.append(0*np.random.rand(sizes[i+1],1))\n",
    "        \n",
    "    len_w = len(weight_matrix_list)  \n",
    "    len_b = len(bias_list)\n",
    "    \n",
    "    eta = 0.01\n",
    "    batch_size = 10\n",
    "    num_points_seen = 0\n",
    "    beta1 , beta2 , eps = 0.9 , 0.999 , 1e-8\n",
    "    epochs = 20\n",
    "    t=0\n",
    "    data_length = len(xtrain)\n",
    "    no_of_batch = int(data_length/batch_size)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        for batch in range(no_of_batch):\n",
    "            dw=[]\n",
    "            db=[]\n",
    "            for i in range(int(num_hidden)+1):\n",
    "                dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "                db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                l = batch_size*batch + i\n",
    "                r = batch_size*batch + i + 1\n",
    "                feedforward(xtrain[l:r,:],num_hidden)\n",
    "                #print(l , r)\n",
    "                backpropagation(y_true[:,l:r])\n",
    "\n",
    "                for j in range(len_w): \n",
    "                    dw[j] += grad_w_k_list[j].transpose()\n",
    "                                    \n",
    "                for j in range(len_b):\n",
    "                    db[j] += grad_b[j]\n",
    "                                \n",
    "            for j in range(len_w): \n",
    "                m_w[j] = beta1*m_w[j] + (1-beta1)*dw[j]\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                m_b[j] = beta1*m_b[j] + (1-beta1)*db[j]    \n",
    "            \n",
    "            for j in range(len_w): \n",
    "                v_w[j] = beta2*v_w[j] + (1-beta2)*(dw[j]**2)\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                v_b[j] = beta2*v_b[j] + (1-beta2)*(db[j]**2)\n",
    "                \n",
    "            for j in range(len_w): \n",
    "                m_w[j] = m_w[j]/(1-math.pow(beta1 , batch + 1))\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                m_b[j] = m_b[j]/(1-math.pow(beta1 , batch + 1))\n",
    "             \n",
    "            for j in range(len_w): \n",
    "                v_w[j] = v_w[j]/(1-math.pow(beta2 , batch + 1))\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                v_b[j] = v_b[j]/(1-math.pow(beta2 , batch + 1))\n",
    "                \n",
    "            for j in range(len_w):\n",
    "                weight_matrix_list[j] -= (eta/np.sqrt(v_w[j] + eps))*m_w[j]\n",
    "            \n",
    "            for j in range(len_b):\n",
    "                bias_list[j] -= (eta/np.sqrt(v_b[j] + eps))*m_b[j]\n",
    "        \n",
    "        print(\"end of epoch\" , t)\n",
    "            \n",
    "#             print(\"batch\", batch)\n",
    "#             y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "#             print(y_hat)\n",
    "                \n",
    "#         print(\"end  epochs :  \", t)\n",
    "#         y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "#         print(y_hat)\n",
    "#     print(bias_list)\n",
    "#     print(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: RuntimeWarning: overflow encountered in true_divide\n",
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:70: RuntimeWarning: overflow encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of epoch 0\n",
      "end of epoch 1\n",
      "end of epoch 2\n",
      "end of epoch 3\n",
      "end of epoch 4\n",
      "end of epoch 5\n",
      "end of epoch 6\n",
      "end of epoch 7\n",
      "end of epoch 8\n",
      "end of epoch 9\n",
      "end of epoch 10\n",
      "end of epoch 11\n",
      "end of epoch 12\n",
      "end of epoch 13\n",
      "end of epoch 14\n",
      "end of epoch 15\n",
      "end of epoch 16\n",
      "end of epoch 17\n",
      "end of epoch 18\n",
      "end of epoch 19\n"
     ]
    }
   ],
   "source": [
    "adam(weight_matrix_list,bias_list,xtrain,num_hidden,sizes)\n",
    "#print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1048692 ]\n",
      " [0.09999369]\n",
      " [0.10451354]\n",
      " [0.09872875]\n",
      " [0.09517743]\n",
      " [0.10335944]\n",
      " [0.09530957]\n",
      " [0.09939531]\n",
      " [0.10105902]\n",
      " [0.09759406]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "def decode_yhat_to_classes(y_hat):\n",
    "    col = len(y_hat[0])\n",
    "    max_val=0\n",
    "    for j in range(col):\n",
    "        max_val = y_hat[0][j]\n",
    "        index=0\n",
    "        for i in range(9):\n",
    "            if(y_hat[i+1][j] > max_val):\n",
    "                y_hat[index][j]=0\n",
    "                index = i+1\n",
    "                max_val=y_hat[i+1][j]\n",
    "            else :\n",
    "                y_hat[i+1][j]=0\n",
    "        y_hat[index][j]=1\n",
    "    return y_hat\n",
    "\n",
    "xtest = pd.read_csv('test.csv').as_matrix()\n",
    "test=xtest[:,1:]\n",
    "\n",
    "xmean = np.mean(test , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "test = test - means_expanded\n",
    "\n",
    "xstd = np.std(test , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "test = test*1.0/std_expanded \n",
    "\n",
    "\n",
    "feedforward(test,num_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06807082 0.06825942 0.06780937 ... 0.06785387 0.06789097 0.06811857]\n",
      " [0.13407689 0.13386117 0.13437845 ... 0.13432678 0.13428027 0.13402486]\n",
      " [0.06923263 0.06941714 0.06897735 ... 0.06902276 0.06905957 0.0692788 ]\n",
      " ...\n",
      " [0.06836437 0.06855471 0.0681053  ... 0.06815119 0.06818811 0.06841243]\n",
      " [0.06756892 0.06776089 0.06730267 ... 0.06734868 0.06738946 0.0676159 ]\n",
      " [0.09623877 0.09628915 0.09616436 ... 0.09617998 0.09619066 0.09625033]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n",
      "[[3]\n",
      " [3]\n",
      " [3]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_to_y_predict(y_predict):\n",
    "    ide=[]\n",
    "    row=len(y_predict)\n",
    "    for i in range(row):\n",
    "        ide.append(i)\n",
    "    ide=np.array(ide)\n",
    "    ide=ide[:,np.newaxis]\n",
    "    y= np.hstack((ide,y_predict))\n",
    "    #print(y_predict, ide)\n",
    "    return y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [6]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [6]\n",
      " [1]]\n",
      "[[   0    6]\n",
      " [   1    6]\n",
      " [   2    1]\n",
      " ...\n",
      " [9997    1]\n",
      " [9998    6]\n",
      " [9999    1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)\n",
    "#y_predict= np.random.randint(10, size=(10000, 1))\n",
    "y_predict= add_id_to_y_predict(y_predict)\n",
    "print(y_predict)\n",
    "x,y = y_predict.shape\n",
    "#print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_from_adam.csv', 'w', newline='') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    row,col=y_predict.shape \n",
    "    thewriter.writerow(['id','label'])\n",
    "    for i in range(row):\n",
    "        thewriter.writerow(y_predict[i])\n",
    "#out.close()\n",
    "#np.savetxt(\"foo.csv\", y_predict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_hat[0]))\n",
    "max(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preactivation_list[3])\n",
    "print(activation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(weight_matrix_list,bias_list,test,hidden_layers):\n",
    "    nos_data = len(test)\n",
    "    #print(nos_data)\n",
    "    #preactivation_list_new=[]\n",
    "    #activation_list_new=[]\n",
    "    #for i in range(nos_data):\n",
    "    h = test.transpose()\n",
    "    #preactivation_list_new.append(h)\n",
    "    #activation_list_new.append(h)\n",
    "    #print(h)\n",
    "    for j in range(hidden_layers):\n",
    "        a_new = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),h)\n",
    "        #preactivation_list_new.append(a_new)\n",
    "        h = 1.0/(1.0+np.exp(-a_new))\n",
    "        #activation_list_new.append(h)\n",
    "        #print(\"next activation\", h)\n",
    "\n",
    "    j=hidden_layers\n",
    "    #hlist.append(h)\n",
    "    a_last = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),h)\n",
    "    #preactivation_list.append(a_last)\n",
    "    print(\"last activation\", a_last)\n",
    "    #y_hat_new = np.array(output(a_last))\n",
    "    #y_hat_new = y_hat_new.transpose()\n",
    "    #print(\"y_hat\", y_hat)\n",
    "    #print(\"y_hat_global\", y_hat_global)\n",
    "    #print(y_hat.transpose())\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([\n",
    "    [5,6,7],\n",
    "    [6,7,8]\n",
    "]\n",
    ")\n",
    "y=test_func(weight_matrix_list,bias_list,test,num_hidden)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")\n",
    "y = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")\n",
    "print(np.subtract(x,y))\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_true), type(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4]\n",
    "print((a[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.tile(np.arange(1,10,10),(10,4))\n",
    "print(s)\n",
    "s[:,i:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=output(s)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.tile(np.arange(1,10,10),(10,4))\n",
    "print(s)\n",
    "print(s[:,i:i+1])\n",
    "p=softmax(s[:,i:i+1].transpose())\n",
    "print(\"P\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dash(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2 1.2 1.2 1.2 1.2]\n",
      " [2.2 2.2 2.2 2.2 2.2]\n",
      " [3.2 3.2 3.2 3.2 3.2]\n",
      " [4.2 4.2 4.2 4.2 4.2]]\n",
      "[1.16619038 1.16619038 1.16619038 1.16619038]\n",
      "[[1.16619038 1.16619038 1.16619038 1.16619038 1.16619038]\n",
      " [1.16619038 1.16619038 1.16619038 1.16619038 1.16619038]\n",
      " [1.16619038 1.16619038 1.16619038 1.16619038 1.16619038]\n",
      " [1.16619038 1.16619038 1.16619038 1.16619038 1.16619038]]\n",
      "[[0.         0.         0.85749293 1.71498585 2.57247878]\n",
      " [0.85749293 0.85749293 1.71498585 2.57247878 3.4299717 ]\n",
      " [1.71498585 1.71498585 2.57247878 3.4299717  4.28746463]\n",
      " [2.57247878 2.57247878 3.4299717  4.28746463 5.14495755]]\n",
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "])\n",
    "#print(type(x))\n",
    "y = np.mean(x , axis=1)\n",
    "means_expanded = np.outer(y, np.ones(5))\n",
    "print(means_expanded)\n",
    "z=x-means_expanded\n",
    "a = np.std(x,axis=1)\n",
    "variance_extended = np.outer(a,np.ones(5))\n",
    "print(a)\n",
    "print(variance_extended)\n",
    "unit_x= x/variance_extended\n",
    "print(unit_x)\n",
    "check = np.std(unit_x, axis=1)\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "mat = np.array([\n",
    "    [0,0,1,2,7],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "])\n",
    "sum_col = np.sum(mat[0:1,:],axis=1)\n",
    "print(sum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(a_last_layer,batch_train):\n",
    "    y_hat_initial=[]\n",
    "    y_hat_local=[]\n",
    "    nos_data=len(batch_train)\n",
    "    for i in range(nos_data):\n",
    "        y_hat_initial.append(softmax(a_last_layer[:,i:i+1].transpose()))\n",
    "\n",
    "    length = len(y_hat_initial)\n",
    "    for i in range(length):\n",
    "        y_hat_local.append(y_hat_initial[i][0])\n",
    "    \n",
    "    print(\"y_hat_local\", y_hat_local)\n",
    "    print(\"y_hat_initial\",y_hat_initial)\n",
    "    return y_hat_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(row_vector):\n",
    "    i = row_vector.size\n",
    "    sum_all=0\n",
    "    row_vector = np.exp(row_vector)\n",
    "    for j in range(i):\n",
    "        sum_all+= row_vector[0][j]\n",
    "    row_vector = (row_vector*1.0/sum_all)\n",
    "    return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "    t=0\n",
    "    max_iteration=10\n",
    "    \n",
    "    eta=0.01\n",
    "    data_length = len(xtrain)\n",
    "    batch_size = 10\n",
    "    #batch_size = data_length\n",
    "    no_of_batch = int(data_length/batch_size)\n",
    "    #no_of_batch =1\n",
    "    while(t < max_iteration):\n",
    "        dw=[]\n",
    "        db=[]\n",
    "        \n",
    "        for i in range(int(num_hidden)+1):\n",
    "            dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "            db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "        \n",
    "        \n",
    "        for batch in range(no_of_batch):\n",
    "            l=batch_size*batch\n",
    "            r=batch_size*batch+batch_size\n",
    "            feedforward(xtrain[l:r,:],num_hidden)\n",
    "            backpropagation(y_true[:,l:r])\n",
    "            \n",
    "            length=len(weight_matrix_list)\n",
    "            i=0\n",
    "            #print(\"grad_w\",grad_w_k_list)\n",
    "            #print(\"a_matrix\",preactivation_list)\n",
    "            #print(\"hactivation\",activation_list)\n",
    "            #print(\"hat\",y_hat)\n",
    "            \n",
    "            while(i<length):\n",
    "                dw[i] += grad_w_k_list[i].transpose()\n",
    "                i+=1\n",
    "                #print(\"weight_matrix_list\", weight_matrix_list[i])\n",
    "                #print(\"grad_w_k_list\", grad_w_k_list[i])\n",
    "                #print(\"weight_matrix_list\", weight_matrix_list[i])\n",
    "                \n",
    "            length=len(bias_list)\n",
    "            i=0\n",
    "            \n",
    "            while(i<length):\n",
    "                db[i] += grad_b[i]\n",
    "                i+=1\n",
    "                #print(\"y_hat\", y_hat)\n",
    "                #y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "                #if(batch == batch_size-1):\n",
    "                #print(\"y_hat\", y_hat)\n",
    "                #print(\"y_hat for batch \", batch , y_hat_local)\n",
    "        \n",
    "        length=len(weight_matrix_list)\n",
    "        i=0\n",
    "        while(i<length):\n",
    "            weight_matrix_list[i] -= eta*dw[i]\n",
    "            i+=1\n",
    "        \n",
    "        length=len(bias_list)\n",
    "        i=0        \n",
    "        while(i<length):\n",
    "            bias_list[i] -= eta*db[i]\n",
    "            i+=1\n",
    "        t+=1\n",
    "        print(\"end  epochs :  \", t)\n",
    "        #print(y_hat)\n",
    "#     print(bias_list)\n",
    "#     print(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw=[]\n",
    "db=[]\n",
    "        \n",
    "for i in range(int(num_hidden)+1):\n",
    "    dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "    db.append(0*np.random.rand(sizes[i+1],1))\n",
    "print(dw)\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  4 49]\n",
      " [ 1  1  4  9 16]\n",
      " [ 4  4  9 16 25]\n",
      " [ 9  9 16 25 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mat = np.array([\n",
    "    [0,0,1,2,7],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "])\n",
    "y = mat**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
