{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import openpyxl\n",
    "import math\n",
    "'''train = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")'''\n",
    "'''\n",
    "train = np.array([\n",
    "    [0,1,1,1,3],\n",
    "    [1,1,1,1,4],\n",
    "    [2,1,1,1,5],\n",
    "    [3,1,1,1,6]\n",
    "]\n",
    ")'''\n",
    "train = pd.read_csv('train.csv').as_matrix()#read train.csv file\n",
    "#print(type(train))\n",
    "#xtrain=train[:,1:4]\n",
    "#xtrain = xtrain/5\n",
    "#true_label=train[:,4:5]\n",
    "#print((xtrain),(true_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_into_prob(x,max_val):\n",
    "    len=x.size\n",
    "    y_list=[]\n",
    "    for i in range(len):\n",
    "        temp=[]\n",
    "        for j in range(max_val):\n",
    "            if(x[i]==j):\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "            #print(temp)\n",
    "        y_list.append(temp)\n",
    "    return y_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55945675 -0.55945675  2.06254185 ...  2.0522595  -0.55945675\n",
      "  -0.55945675]\n",
      " [-0.5380259  -0.5380259  -0.5380259  ... -0.5380259  -0.5380259\n",
      "  -0.5380259 ]\n",
      " [-0.6486501  -0.6486501  -0.6486501  ... -0.6486501  -0.6486501\n",
      "  -0.6486501 ]\n",
      " ...\n",
      " [-0.64973918 -0.64973918 -0.64973918 ... -0.64973918 -0.64973918\n",
      "  -0.64973918]\n",
      " [-0.59745866 -0.59745866 -0.59745866 ... -0.59745866 -0.59745866\n",
      "  -0.59745866]\n",
      " [-0.62629522 -0.62629522 -0.62629522 ... -0.62629522 -0.62629522\n",
      "  -0.62629522]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "xtrain=train[:,1:785]\n",
    "\n",
    "xmean = np.mean(xtrain , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "xtrain = xtrain - means_expanded\n",
    "\n",
    "xstd = np.std(xtrain , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "xtrain = xtrain*1.0/std_expanded \n",
    "\n",
    "true_label = train[:,785:]\n",
    "y_true=np.array(encode_into_prob(true_label,10))\n",
    "y_true=y_true.transpose()\n",
    "\n",
    "#print(type(y_true))\n",
    "#print(y_true.transpose())\n",
    "print(xtrain)\n",
    "print((y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(batch_train,hidden_layers):\n",
    "     \n",
    "    global weight_matrix_list\n",
    "    global bias_list\n",
    "    global preactivation_list \n",
    "    preactivation_list=[]\n",
    "    global activation_list\n",
    "    activation_list=[]\n",
    "    global y_hat\n",
    "    y_hat=[]\n",
    "    a_new=[]\n",
    "    a_last_layer=[]\n",
    "    nos_data = len(batch_train)\n",
    "    #print(nos_data)\n",
    "    h=batch_train.transpose()\n",
    "    preactivation_list.append(h)\n",
    "    activation_list.append(h)\n",
    "    #print(h)\n",
    "    \n",
    "#     print(\"y_hat in feed \\n\",y_hat )\n",
    "    \n",
    "    for j in range(hidden_layers):\n",
    "        \n",
    "#         print(\".............................\")\n",
    "#         print(\"activation for layer \", j)\n",
    "#         print(activation_list[j] , activation_list[j].shape)\n",
    "#         print(\"weight\",j+1)\n",
    "#         print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#         print(\"bias \",j+1)\n",
    "#         print(bias_list[j],bias_list[j].shape)\n",
    "        \n",
    "        a_new = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "        preactivation_list.append(a_new)\n",
    "        \n",
    "#         print(\"preactivation for layer\",j+1)\n",
    "#         print(preactivation_list[j+1], preactivation_list[j+1].shape)\n",
    "        \n",
    "        h = 1.0/(1.0+np.exp(-1*preactivation_list[j+1]))\n",
    "        activation_list.append(h)\n",
    "\n",
    "    \n",
    "    j=hidden_layers\n",
    "#     print(\".............................\")\n",
    "#     print(\"activation for layer \", j)\n",
    "#     print(activation_list[j] , activation_list[j].shape)\n",
    "#     print(\"weight\",j+1)\n",
    "#     print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#     print(\"bias \",j+1)\n",
    "#     print(bias_list[j],bias_list[j].shape)\n",
    "    \n",
    "    a_last_layer = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "    preactivation_list.append(a_last_layer)\n",
    "    \n",
    "#     print(\"last activation\")\n",
    "#     print(a_last_layer,a_last_layer.shape)\n",
    "\n",
    "    y_hat = np.array(output(a_last_layer,batch_train))\n",
    "    y_hat = y_hat.transpose()\n",
    "    \n",
    "#     print(\"y_hat in feedforward end\")\n",
    "#     print(y_hat)\n",
    "    \n",
    "    \n",
    "def output(a_last_layer,batch_train):\n",
    "    y_hat_initial=[]\n",
    "    y_hat_local=[]\n",
    "    nos_data=len(batch_train)\n",
    "    for i in range(nos_data):\n",
    "        y_hat_initial.append(softmax(a_last_layer[:,i:i+1].transpose()))\n",
    "\n",
    "    length = len(y_hat_initial)\n",
    "    for i in range(length):\n",
    "        y_hat_local.append(y_hat_initial[i][0])\n",
    "    \n",
    "#     print(\"y_hat_local\", y_hat_local)\n",
    "#     print(\"y_hat_initial\",y_hat_initial)\n",
    "    return y_hat_local\n",
    "\n",
    "def softmax(row_vector):\n",
    "    sum_all=0\n",
    "    row_vector = np.exp(row_vector)\n",
    "    sum_all = np.sum(row_vector , axis=1)         #print(\"row_vector\", row_vector)\n",
    "    row_vector = (row_vector*1.0/sum_all)         #print(\"sum_all\",sum_all)\n",
    "    return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(y_true_batch):\n",
    "    \n",
    "#     print(\"weigt in backpropogation\", weight_matrix_list)\n",
    "#     print(\"...........y_true_batch..........\\n\",y_true_batch)\n",
    "    \n",
    "    global grad_a_k_list\n",
    "    grad_a_k_list=[]\n",
    "    \n",
    "    global grad_w_k_list\n",
    "    grad_w_k_list=[]\n",
    "    \n",
    "    global grad_h_k_list\n",
    "    grad_h_k_list=[]\n",
    "    \n",
    "    global grad_b_k_list\n",
    "    grad_b_k_list=[]\n",
    "    \n",
    "    global grad_b\n",
    "    grad_b=[]\n",
    "    \n",
    "    global grad_g_dash_a_k_list\n",
    "    grad_g_dash_a_k_list = []\n",
    "    \n",
    "    global grad_a_last\n",
    "    grad_a_last=[]\n",
    "    \n",
    "    k = num_hidden+1\n",
    "    \n",
    "#     print(\"--------------------------in Backpropagation---------------------\")\n",
    "    \n",
    "    grad_g_dash_a_k_list = g_dash(preactivation_list)\n",
    "    \n",
    "#     print(\"preactivation_list....................\")\n",
    "#     print(\"Nos. of matrices in preactivation_list is\", len(preactivation_list))\n",
    "#     print(preactivation_list)\n",
    "#     print(\"g_dash..................\")\n",
    "#     print(\"Nos. of matrices in g_dash is \", len(grad_g_dash_a_k_list))\n",
    "#     print(grad_g_dash_a_k_list)\n",
    "    \n",
    "    grad_a_last = -np.subtract(y_true_batch,y_hat)\n",
    "    \n",
    "#     print(\"y_true_batch \\n\", y_true_batch)\n",
    "#     print(\"y_hat \\n\",y_hat)\n",
    "    \n",
    "    grad_a_k_list.insert(0,grad_a_last)\n",
    "    \n",
    "#     print(\"grad_a_last_layer\", k)\n",
    "#     print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "  \n",
    "    \n",
    "    while(k>0):\n",
    "        \n",
    "#         print(\".................grad_compute at layer\",k)\n",
    "        \n",
    "        grad_w_k_list.insert(0,np.matmul(grad_a_k_list[0],activation_list[k-1].transpose()))\n",
    "        \n",
    "#         print(\"grad_a\", k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print('preactivation  a_',k-1)\n",
    "#         print(preactivation_list[k-1],preactivation_list[k-1].shape)\n",
    "#         print(\"grad_w_\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "        \n",
    "        grad_b_k_list.insert(0,grad_a_k_list[0])\n",
    "        \n",
    "#         print(\"grad_b\",k)\n",
    "#         print(grad_b_k_list[0],grad_b_k_list[0].shape)\n",
    "        \n",
    "        grad_h_k_list.insert(0,np.matmul(weight_matrix_list[k-1],grad_a_k_list[0]))\n",
    "        \n",
    "#         print(\"grad_w\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "#         print(\"grad_a\",k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print(\"grad_h\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "        \n",
    "        grad_a_k_list.insert(0,np.multiply(grad_h_k_list[0],grad_g_dash_a_k_list[k-1]))\n",
    "        \n",
    "#         print(\"grad_h_k_list[0]\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "#         print(\"grad_g_dash_a_k_list[k-1]\",k-1)\n",
    "#         print(grad_g_dash_a_k_list[k-1],grad_g_dash_a_k_list[k-1].shape)\n",
    "#         print(\"grad_a\",k-1)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "        k=k-1\n",
    "#         print(\"grad_a_ik_list\",grad_a_k_list[0])\n",
    "    \n",
    "    sum_all_grad_b_of_data(grad_b_k_list)\n",
    "    \n",
    "\n",
    "def sum_all_grad_b_of_data(grad_b_k_list):\n",
    "    length=len(grad_b_k_list)\n",
    "    i=0\n",
    "    while(i<length):\n",
    "        row=[]\n",
    "        row = np.sum(grad_b_k_list[i],1)\n",
    "        j = len(row)\n",
    "        #print(row.shape)\n",
    "        row = row[:, np.newaxis]\n",
    "        #print(row.shape , row)\n",
    "        grad_b.append(row)\n",
    "        i+=1\n",
    "    #print(\"grad_b\",grad_b)\n",
    "\n",
    "def g_dash(preactivation_list):\n",
    "    length=len(preactivation_list)\n",
    "    a_modified = preactivation_list[0:length]\n",
    "    g_dash_a_k_list=[]\n",
    "    for i in range(length):\n",
    "        a=(1.0/(1+np.exp(-a_modified[i])))*(1-(1.0/(1+np.exp(-a_modified[i]))))\n",
    "        g_dash_a_k_list.append(a)\n",
    "    return g_dash_a_k_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_yhat_to_classes(y_hat_local):\n",
    "    col = len(y_hat_local[0])\n",
    "    max_val=0\n",
    "    for j in range(col):\n",
    "        max_val = y_hat_local[0][j]\n",
    "        index=0\n",
    "        for i in range(9):\n",
    "            if(y_hat_local[i+1][j] > max_val):\n",
    "                y_hat_local[index][j]=0\n",
    "                index = i+1\n",
    "                max_val=y_hat[i+1][j]\n",
    "            else :\n",
    "                y_hat_local[i+1][j]=0\n",
    "        y_hat_local[index][j]=1\n",
    "    return y_hat_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i+1)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 50, 100, 10]\n",
      "4\n",
      "[array([[0.02877622, 0.00612583, 0.01060217, ..., 0.02232832, 0.01609823,\n",
      "        0.03240143],\n",
      "       [0.01916133, 0.02359844, 0.01224246, ..., 0.03169097, 0.01498393,\n",
      "        0.02119273],\n",
      "       [0.0004331 , 0.03338946, 0.01581992, ..., 0.00632437, 0.0148531 ,\n",
      "        0.03523695],\n",
      "       ...,\n",
      "       [0.01248194, 0.00885848, 0.03438131, ..., 0.0148197 , 0.00449166,\n",
      "        0.02872777],\n",
      "       [0.02863293, 0.02627378, 0.02268934, ..., 0.01922257, 0.0145979 ,\n",
      "        0.00309374],\n",
      "       [0.00080867, 0.00818828, 0.03267851, ..., 0.01728382, 0.02359067,\n",
      "        0.00057805]]), array([[0.0704615 , 0.13309439, 0.10910513, ..., 0.10945406, 0.13304574,\n",
      "        0.08807632],\n",
      "       [0.07408403, 0.11719756, 0.1272977 , ..., 0.11134738, 0.09231599,\n",
      "        0.00299497],\n",
      "       [0.00287579, 0.13030375, 0.02938027, ..., 0.10366056, 0.00156677,\n",
      "        0.10191214],\n",
      "       ...,\n",
      "       [0.09350648, 0.12471002, 0.07459373, ..., 0.12564391, 0.01299333,\n",
      "        0.13148649],\n",
      "       [0.03905418, 0.00129376, 0.02616093, ..., 0.11896684, 0.09573252,\n",
      "        0.09366482],\n",
      "       [0.13410843, 0.06723408, 0.04142462, ..., 0.08234852, 0.10724236,\n",
      "        0.09071039]]), array([[4.79205392e-02, 8.59766626e-02, 6.05355947e-03, 8.29524563e-02,\n",
      "        1.68006415e-04, 5.66084107e-02, 9.21084943e-02, 1.92773633e-02,\n",
      "        8.26173579e-02, 1.56854157e-06],\n",
      "       [9.06104440e-02, 1.63605538e-02, 7.12246117e-02, 8.53311989e-02,\n",
      "        3.58565104e-02, 5.02084705e-02, 3.05999585e-03, 9.05128431e-02,\n",
      "        4.01700744e-02, 7.03268965e-02],\n",
      "       [9.37439072e-02, 4.80366206e-02, 6.58103897e-02, 6.93884720e-02,\n",
      "        9.86126350e-02, 8.63957803e-02, 9.33221379e-02, 3.91430472e-02,\n",
      "        2.18217413e-02, 6.70876698e-02],\n",
      "       [7.70292430e-02, 1.25645162e-02, 9.29820959e-02, 6.68769956e-02,\n",
      "        5.31555252e-02, 1.84836762e-02, 7.81159828e-02, 3.14302086e-02,\n",
      "        1.10179711e-02, 6.52595041e-02],\n",
      "       [1.10052568e-02, 3.14017228e-02, 5.24106601e-02, 1.21290079e-02,\n",
      "        9.68433828e-02, 6.76838027e-02, 2.61685686e-03, 9.88836323e-02,\n",
      "        2.08407570e-02, 2.43579863e-02],\n",
      "       [8.00847371e-02, 2.89289507e-02, 9.21270947e-03, 6.86845160e-02,\n",
      "        1.25107385e-02, 6.45083013e-02, 5.73852404e-02, 7.93963918e-02,\n",
      "        3.56002707e-02, 9.65335228e-02],\n",
      "       [5.46052397e-02, 6.46943874e-02, 8.97309957e-02, 7.62648591e-02,\n",
      "        9.73610422e-02, 8.40266592e-02, 7.87300036e-02, 5.59372041e-02,\n",
      "        4.50927259e-02, 3.46323538e-02],\n",
      "       [4.44962397e-02, 5.63927025e-03, 2.86032171e-02, 6.23858127e-02,\n",
      "        1.27881574e-02, 9.14071454e-02, 7.97870190e-02, 2.67703763e-02,\n",
      "        5.40755409e-02, 5.94038882e-02],\n",
      "       [5.52902543e-02, 7.06142659e-02, 3.87455016e-02, 6.20240441e-02,\n",
      "        6.46024305e-02, 5.94991830e-02, 4.54874538e-02, 1.80336695e-02,\n",
      "        3.69752109e-03, 8.01588478e-02],\n",
      "       [5.04749985e-02, 8.06460465e-02, 8.24864603e-02, 5.11824625e-02,\n",
      "        6.13600264e-02, 4.20269791e-02, 5.71434314e-03, 1.36573119e-02,\n",
      "        2.77756061e-02, 2.68297782e-02],\n",
      "       [5.56255296e-02, 2.69796073e-02, 4.94723532e-02, 7.83255319e-02,\n",
      "        1.90000403e-02, 3.93156328e-03, 8.43174075e-02, 3.24102352e-02,\n",
      "        6.82460479e-02, 9.33078979e-02],\n",
      "       [4.96451933e-02, 2.03865570e-02, 5.91871482e-02, 3.20329304e-02,\n",
      "        5.61655460e-03, 3.87840671e-02, 3.53014641e-02, 8.17806413e-03,\n",
      "        9.75879910e-02, 2.05333328e-02],\n",
      "       [4.90616231e-02, 6.92331436e-02, 3.06115550e-02, 5.51488453e-03,\n",
      "        5.49489584e-02, 2.49136538e-03, 9.47250684e-02, 1.24120626e-02,\n",
      "        7.77597176e-02, 2.68693974e-02],\n",
      "       [9.98960224e-02, 5.41952627e-02, 9.78276636e-02, 4.59451152e-02,\n",
      "        5.53952837e-02, 2.35928163e-02, 3.69131875e-02, 2.16034747e-02,\n",
      "        1.14832804e-02, 3.99596203e-03],\n",
      "       [2.00304018e-02, 4.02251965e-02, 6.85474093e-02, 2.58371987e-02,\n",
      "        4.27238587e-02, 6.84259912e-02, 5.59247852e-03, 7.06012317e-02,\n",
      "        3.09566310e-02, 2.13841134e-02],\n",
      "       [7.58655291e-02, 9.89484094e-04, 5.23778002e-02, 7.65168420e-02,\n",
      "        4.47504388e-03, 3.44700850e-02, 8.15831027e-02, 7.61956492e-02,\n",
      "        2.44606220e-02, 9.63350169e-02],\n",
      "       [8.05030586e-02, 5.67769526e-02, 5.70855507e-03, 3.50693947e-02,\n",
      "        1.86573278e-02, 2.40992407e-02, 9.02441546e-02, 7.75464113e-02,\n",
      "        2.77741447e-02, 4.09177373e-02],\n",
      "       [1.46811433e-02, 6.21851512e-02, 8.32541981e-02, 1.11347886e-02,\n",
      "        1.91795905e-02, 7.13374028e-02, 7.13466425e-02, 5.64238899e-02,\n",
      "        1.24617233e-02, 9.35824074e-02],\n",
      "       [6.56266190e-02, 6.40007488e-02, 8.12003490e-02, 1.82245858e-02,\n",
      "        4.67261559e-02, 1.98184967e-02, 4.87037626e-02, 9.83064589e-02,\n",
      "        4.57032043e-02, 3.20394939e-02],\n",
      "       [2.91646139e-02, 5.25477703e-02, 2.85036338e-02, 9.32263584e-02,\n",
      "        9.05319147e-02, 5.97482619e-02, 1.46823588e-02, 9.58859513e-02,\n",
      "        8.60427397e-02, 3.59973905e-02],\n",
      "       [8.23339361e-02, 5.53464731e-02, 4.48383925e-02, 2.85399374e-02,\n",
      "        8.86365525e-02, 3.08683374e-02, 7.68162634e-02, 9.73378636e-02,\n",
      "        6.29115209e-02, 5.91072342e-02],\n",
      "       [5.25976400e-02, 8.19942193e-02, 3.56228341e-02, 1.72713316e-02,\n",
      "        2.73899338e-02, 7.77680008e-02, 2.83963459e-03, 7.59806509e-02,\n",
      "        4.24151101e-02, 4.33874063e-02],\n",
      "       [2.85205812e-02, 1.94294221e-02, 4.92639825e-02, 2.03775446e-02,\n",
      "        9.95077646e-02, 6.27435921e-02, 9.04041846e-02, 3.20195203e-02,\n",
      "        6.09056136e-02, 7.69503866e-03],\n",
      "       [5.77058764e-02, 7.55101838e-02, 3.36500259e-02, 4.01332002e-02,\n",
      "        6.88837920e-02, 4.24266224e-02, 4.64665662e-02, 6.25595642e-02,\n",
      "        6.01804888e-02, 9.60845067e-02],\n",
      "       [1.53682585e-02, 3.96202803e-02, 1.32700173e-02, 8.00104430e-02,\n",
      "        1.00078715e-02, 6.13623226e-02, 5.20028363e-03, 4.69204645e-02,\n",
      "        8.05181810e-02, 3.16590152e-02],\n",
      "       [5.95756526e-02, 6.51432098e-03, 5.71711033e-02, 8.03124982e-02,\n",
      "        5.75791632e-02, 5.40669898e-02, 7.60145457e-02, 2.01773444e-02,\n",
      "        9.60411369e-02, 6.73241402e-02],\n",
      "       [4.42389427e-02, 1.36463448e-02, 4.67820031e-03, 8.12427489e-02,\n",
      "        5.32865320e-02, 7.87702939e-02, 8.41278652e-02, 5.87608022e-02,\n",
      "        8.14878770e-02, 6.49717835e-02],\n",
      "       [4.49687608e-02, 2.49489811e-02, 8.83396304e-02, 1.60908233e-02,\n",
      "        7.38451009e-02, 9.10759346e-02, 2.32617409e-02, 9.89181364e-02,\n",
      "        6.23554393e-02, 6.05243658e-02],\n",
      "       [1.52331941e-02, 3.37878251e-03, 8.60150933e-02, 9.50422272e-03,\n",
      "        2.09564685e-03, 6.81395421e-03, 7.42514385e-02, 8.68144525e-02,\n",
      "        4.18099429e-02, 1.38487448e-03],\n",
      "       [2.79941409e-02, 5.88225894e-02, 6.22783949e-02, 5.57139708e-02,\n",
      "        8.68978407e-02, 9.45135117e-02, 9.95547137e-02, 5.45019249e-02,\n",
      "        3.86502503e-02, 3.49594090e-02],\n",
      "       [3.05570186e-04, 7.68424413e-02, 6.45482396e-02, 2.91355188e-02,\n",
      "        6.62248205e-02, 3.62202374e-02, 7.82312768e-02, 1.14700662e-02,\n",
      "        2.05234431e-02, 2.73759992e-02],\n",
      "       [9.96104396e-02, 9.04968182e-02, 4.61431884e-02, 7.48286424e-02,\n",
      "        2.95446326e-02, 3.00898878e-02, 7.53046560e-02, 2.02416701e-02,\n",
      "        8.02350409e-02, 1.92820835e-02],\n",
      "       [8.93453210e-02, 6.42146173e-02, 4.79546472e-02, 5.32749729e-02,\n",
      "        5.84955290e-02, 9.00512242e-02, 2.22349192e-02, 7.99505046e-02,\n",
      "        9.12285866e-02, 6.74329419e-02],\n",
      "       [6.73025413e-02, 3.68113553e-02, 5.05959970e-02, 8.05844469e-02,\n",
      "        3.82950116e-03, 1.77032647e-02, 5.98770943e-02, 8.84525304e-02,\n",
      "        4.79232532e-03, 6.81612663e-02],\n",
      "       [3.12116578e-02, 3.95104291e-02, 9.49925152e-02, 1.76147256e-02,\n",
      "        2.05779168e-03, 7.67163777e-02, 6.86036775e-02, 3.95418419e-02,\n",
      "        7.08864530e-02, 2.16913819e-02],\n",
      "       [2.06448464e-02, 6.22792010e-03, 7.25776103e-03, 4.64395944e-02,\n",
      "        8.85699967e-02, 1.37375345e-02, 7.11861704e-02, 8.63283854e-02,\n",
      "        5.22680463e-02, 3.49778600e-02],\n",
      "       [5.62992941e-02, 8.37559687e-02, 1.67635775e-02, 6.92846493e-02,\n",
      "        6.59315470e-02, 5.15328197e-02, 7.70704733e-02, 6.14672777e-02,\n",
      "        4.37128019e-02, 4.25292131e-02],\n",
      "       [5.69743819e-02, 4.39232330e-02, 4.18581164e-02, 8.51395300e-02,\n",
      "        3.15714361e-02, 3.07921345e-02, 5.23529037e-02, 7.40302353e-02,\n",
      "        4.27410749e-02, 1.34693669e-02],\n",
      "       [4.81457053e-02, 9.67259368e-02, 2.62156136e-02, 4.48556637e-02,\n",
      "        8.01885327e-02, 2.55602855e-02, 4.59540917e-03, 3.63256814e-02,\n",
      "        3.75442079e-02, 1.61858414e-02],\n",
      "       [1.32380780e-02, 1.87033328e-03, 8.99358048e-02, 6.39126524e-02,\n",
      "        4.35984955e-02, 9.70304964e-02, 6.07841570e-04, 3.34889524e-02,\n",
      "        8.06048645e-02, 5.72537134e-02],\n",
      "       [7.41947536e-02, 4.80978857e-02, 6.34313946e-02, 8.66829504e-03,\n",
      "        9.87923204e-02, 9.25946380e-02, 5.85108476e-02, 6.87376583e-04,\n",
      "        6.83600929e-02, 8.35206267e-02],\n",
      "       [3.75571513e-03, 2.54706562e-02, 1.79089955e-02, 9.93427432e-02,\n",
      "        8.77382184e-02, 4.08257746e-02, 3.62468781e-02, 2.00352998e-02,\n",
      "        8.25189998e-02, 7.43793020e-02],\n",
      "       [7.01098062e-03, 4.36562976e-02, 4.53860317e-02, 6.98445415e-04,\n",
      "        2.64567480e-03, 5.34581257e-03, 1.77049649e-02, 6.02131591e-02,\n",
      "        6.21858890e-02, 6.48061862e-02],\n",
      "       [9.43863749e-03, 5.26961195e-03, 4.12960057e-02, 3.75794858e-02,\n",
      "        9.76756246e-02, 1.55715993e-03, 4.33058352e-02, 9.57705427e-02,\n",
      "        7.88311930e-02, 5.01680963e-02],\n",
      "       [1.46853815e-02, 7.04837122e-02, 5.55287804e-02, 2.98478622e-02,\n",
      "        6.67220658e-02, 1.90898246e-02, 4.96187158e-02, 7.19398964e-02,\n",
      "        8.03610864e-02, 3.57951965e-02],\n",
      "       [4.81253235e-02, 4.24714397e-02, 7.76880271e-02, 5.16240318e-02,\n",
      "        9.58211928e-03, 3.57711885e-02, 4.64001877e-02, 5.58765448e-02,\n",
      "        7.41748906e-02, 6.78437288e-02],\n",
      "       [4.17296622e-02, 7.57512354e-03, 8.63972221e-02, 8.16922507e-02,\n",
      "        6.31652729e-02, 7.97397385e-02, 9.05869632e-02, 7.28877967e-02,\n",
      "        1.09120078e-02, 7.37473307e-02],\n",
      "       [8.46571549e-02, 5.56981975e-02, 1.68014227e-02, 1.58522445e-02,\n",
      "        7.88962680e-02, 5.10607492e-03, 1.17825231e-02, 1.71749792e-02,\n",
      "        1.11369713e-02, 5.97684885e-02],\n",
      "       [8.23655526e-02, 7.61782763e-02, 5.22001190e-02, 9.04327193e-02,\n",
      "        6.80181259e-02, 7.46093639e-02, 9.11601355e-02, 9.08105116e-02,\n",
      "        9.52086998e-02, 6.76607404e-02],\n",
      "       [8.66779731e-03, 3.13600704e-02, 8.34364202e-02, 5.32111219e-02,\n",
      "        1.66314618e-02, 5.50853363e-02, 2.98651606e-02, 3.32668505e-02,\n",
      "        7.26774195e-02, 1.21329702e-03],\n",
      "       [2.19902252e-02, 9.43308978e-02, 6.98122475e-02, 7.37070785e-02,\n",
      "        4.34069098e-02, 2.08999097e-02, 6.30191495e-02, 2.49841606e-02,\n",
      "        9.46788065e-02, 8.57081484e-02],\n",
      "       [7.77499787e-02, 6.36812035e-03, 8.55466867e-02, 6.61760865e-02,\n",
      "        4.30945593e-02, 9.96851812e-03, 1.03990120e-02, 2.44014494e-03,\n",
      "        2.67216735e-02, 9.92561871e-02],\n",
      "       [3.58982204e-02, 1.98839251e-02, 2.60552052e-03, 4.14346053e-02,\n",
      "        1.05087850e-02, 1.68907496e-02, 3.24698288e-02, 9.11057270e-02,\n",
      "        3.20794282e-02, 9.50620171e-02],\n",
      "       [9.67776427e-02, 7.22903278e-02, 7.84162264e-02, 9.81372990e-02,\n",
      "        9.04322652e-02, 4.70279563e-02, 3.40160363e-02, 6.09698646e-02,\n",
      "        4.04676611e-02, 5.10554319e-03],\n",
      "       [3.14295535e-02, 9.91348681e-02, 8.65173787e-02, 2.14912360e-02,\n",
      "        5.24215773e-02, 8.97402611e-02, 3.54527164e-02, 9.45145995e-02,\n",
      "        2.86597300e-02, 6.55735243e-02],\n",
      "       [1.90503812e-02, 5.70163188e-02, 5.69818627e-02, 8.70328462e-02,\n",
      "        7.27392343e-03, 4.85220960e-02, 4.32388463e-02, 6.73323410e-02,\n",
      "        4.06049276e-02, 3.64215614e-02],\n",
      "       [6.76605196e-02, 3.31518390e-02, 8.37353998e-02, 9.71752048e-02,\n",
      "        4.25828986e-02, 3.64885789e-02, 5.31895546e-03, 8.89305721e-02,\n",
      "        2.46040378e-02, 3.23078606e-03],\n",
      "       [2.18244417e-02, 2.09012339e-03, 2.95259256e-02, 3.78236421e-02,\n",
      "        4.23181599e-02, 1.89037910e-02, 8.26406240e-02, 6.51181149e-02,\n",
      "        9.65959079e-02, 3.02528629e-02],\n",
      "       [2.29354856e-02, 6.39362901e-02, 3.22609514e-02, 9.61607412e-02,\n",
      "        6.27253003e-02, 7.07086687e-03, 2.40169025e-02, 5.13352573e-02,\n",
      "        9.74197486e-02, 1.72819969e-02],\n",
      "       [2.68673918e-02, 2.50218441e-02, 8.01240683e-02, 9.97482808e-02,\n",
      "        8.81334278e-02, 2.68938966e-02, 8.66918260e-02, 3.62219244e-02,\n",
      "        9.13502639e-03, 2.03009291e-02],\n",
      "       [7.57076043e-02, 8.00837199e-02, 8.34346919e-02, 8.75457995e-02,\n",
      "        5.20143749e-02, 6.60693448e-02, 2.89346445e-02, 6.43245253e-02,\n",
      "        3.40009943e-03, 1.54889517e-02],\n",
      "       [5.68256136e-02, 1.02092525e-02, 4.96655176e-02, 1.52639640e-02,\n",
      "        6.62951141e-02, 6.94691457e-02, 9.91688706e-02, 3.57779399e-02,\n",
      "        5.53662952e-02, 7.99542964e-02],\n",
      "       [1.02296484e-02, 7.88366280e-02, 6.79596899e-02, 9.43733297e-03,\n",
      "        5.05855598e-02, 2.82487069e-02, 2.66923835e-02, 4.87284832e-02,\n",
      "        7.65391405e-02, 2.61128622e-02],\n",
      "       [8.20995464e-02, 7.75701106e-02, 2.39936734e-02, 1.99158702e-02,\n",
      "        7.35584466e-02, 3.00642166e-02, 1.99463109e-02, 3.52458006e-02,\n",
      "        2.65381403e-02, 9.15457189e-02],\n",
      "       [5.27166094e-02, 5.41011316e-02, 7.27362103e-02, 8.86874076e-02,\n",
      "        4.74163295e-02, 1.10124006e-02, 2.69651009e-02, 9.06680492e-02,\n",
      "        7.55191712e-02, 1.98479405e-02],\n",
      "       [6.15827989e-02, 5.23399006e-02, 5.25458136e-02, 7.32203170e-02,\n",
      "        6.79173984e-02, 1.38446669e-02, 8.30323063e-02, 4.90412956e-03,\n",
      "        9.30386362e-02, 5.42311695e-02],\n",
      "       [1.36967122e-02, 2.00538153e-02, 3.36388182e-03, 7.31782058e-02,\n",
      "        3.91190518e-02, 2.36817233e-02, 9.68483131e-02, 3.76695633e-02,\n",
      "        2.71542016e-02, 1.95641614e-02],\n",
      "       [4.16810799e-02, 1.83099288e-02, 2.89805518e-02, 5.54384291e-02,\n",
      "        7.54366609e-02, 1.60090238e-02, 5.46354069e-02, 3.14879256e-02,\n",
      "        3.77343054e-02, 6.06340106e-02],\n",
      "       [5.74780394e-02, 7.83718013e-02, 6.01283886e-02, 1.95023885e-02,\n",
      "        2.03197156e-02, 5.61780816e-02, 9.90424658e-02, 4.04133298e-02,\n",
      "        8.37119329e-02, 9.12812833e-02],\n",
      "       [1.86308734e-02, 8.24746510e-02, 7.18732340e-02, 2.60183589e-02,\n",
      "        4.42999176e-02, 2.83749865e-02, 8.90046406e-02, 5.22266713e-02,\n",
      "        4.46767732e-02, 7.86641805e-02],\n",
      "       [1.56701858e-03, 2.43627983e-02, 8.12210081e-03, 7.13848096e-02,\n",
      "        2.36415423e-02, 3.47567679e-02, 9.86833602e-02, 4.85238460e-02,\n",
      "        6.82951062e-02, 1.61293964e-02],\n",
      "       [9.01154548e-02, 3.39693618e-02, 5.05490397e-02, 3.59551704e-03,\n",
      "        1.17110158e-02, 5.33885040e-02, 9.36452179e-02, 7.79369671e-03,\n",
      "        6.38546031e-03, 7.81438027e-02],\n",
      "       [1.45439746e-02, 4.58641686e-02, 1.02052671e-02, 6.06710707e-02,\n",
      "        1.40632655e-02, 4.17418431e-02, 1.16729121e-02, 5.72339175e-02,\n",
      "        9.77586459e-02, 7.62514792e-02],\n",
      "       [8.11293051e-02, 8.40659041e-02, 9.54080140e-02, 7.06751241e-02,\n",
      "        8.64759307e-02, 5.37953448e-02, 2.29500890e-02, 4.32032612e-02,\n",
      "        4.35733370e-03, 9.74386697e-02],\n",
      "       [9.92402377e-02, 5.69192688e-02, 2.65674659e-02, 9.04708404e-02,\n",
      "        2.20048523e-02, 4.05882670e-02, 2.23998685e-02, 7.17639093e-02,\n",
      "        9.55100125e-02, 4.51508542e-02],\n",
      "       [2.29827948e-02, 3.66816756e-02, 8.74884489e-02, 5.98029228e-02,\n",
      "        9.38050951e-02, 9.20699914e-02, 7.17844659e-02, 3.20551488e-02,\n",
      "        4.01743826e-02, 6.00911168e-02],\n",
      "       [3.81232005e-02, 6.32378604e-02, 1.70426655e-02, 9.84661494e-02,\n",
      "        5.31747750e-02, 2.00876736e-02, 7.42533485e-03, 4.97918478e-02,\n",
      "        7.31402176e-02, 9.49356551e-02],\n",
      "       [7.18201486e-02, 4.08281237e-02, 5.69980082e-02, 2.18945536e-02,\n",
      "        4.44224018e-02, 3.94133789e-02, 6.48239562e-02, 2.37616548e-02,\n",
      "        2.16102739e-02, 4.56483090e-02],\n",
      "       [5.07105402e-02, 7.15473391e-02, 2.88642416e-02, 2.40223418e-02,\n",
      "        2.28276067e-02, 1.14289903e-02, 8.03608765e-02, 1.45585667e-03,\n",
      "        3.08984313e-02, 9.79783080e-02],\n",
      "       [2.64448405e-02, 2.75244438e-02, 8.20697184e-03, 3.41153659e-02,\n",
      "        5.20248396e-02, 4.66973809e-02, 8.42101467e-02, 5.26497297e-02,\n",
      "        3.21037086e-02, 7.57786621e-02],\n",
      "       [8.83080479e-02, 3.78079364e-02, 5.93123076e-02, 2.27213281e-02,\n",
      "        9.18359767e-02, 6.45558022e-02, 1.44003050e-02, 6.99147272e-02,\n",
      "        7.84726884e-02, 2.35149930e-02],\n",
      "       [3.85064967e-02, 1.32882402e-02, 8.91500193e-02, 4.52880159e-02,\n",
      "        7.14900514e-02, 9.38351959e-02, 6.75303960e-02, 9.96293729e-02,\n",
      "        5.18208442e-02, 9.61296107e-02],\n",
      "       [7.41252538e-02, 8.58489579e-02, 3.56184237e-02, 4.05574541e-02,\n",
      "        5.19118879e-02, 8.60890522e-02, 4.91371514e-02, 6.17856758e-02,\n",
      "        5.79023039e-02, 1.47334715e-02],\n",
      "       [2.23547000e-02, 5.06131864e-02, 3.12217632e-02, 2.48895057e-02,\n",
      "        7.58738615e-03, 3.08825288e-03, 3.88601537e-02, 1.03165907e-02,\n",
      "        3.69796102e-02, 1.35013288e-02],\n",
      "       [2.78653761e-02, 5.12962095e-02, 6.35388163e-02, 8.38936573e-02,\n",
      "        5.65255432e-04, 3.29986718e-02, 2.99587830e-02, 8.45945898e-02,\n",
      "        6.20337709e-02, 2.92669684e-02],\n",
      "       [8.38705300e-02, 5.31391349e-02, 7.61290576e-02, 1.14857255e-02,\n",
      "        9.67447094e-02, 9.82616593e-03, 2.37132802e-02, 2.82731882e-02,\n",
      "        6.86465381e-02, 4.87593755e-02],\n",
      "       [8.56289466e-03, 2.93149321e-02, 4.95717945e-02, 3.58668992e-02,\n",
      "        5.65413506e-02, 9.40941632e-02, 6.70326846e-02, 3.44849144e-02,\n",
      "        1.88686669e-02, 4.54185576e-02],\n",
      "       [2.14766924e-02, 1.10550186e-02, 4.70959212e-02, 9.31068318e-02,\n",
      "        1.03489987e-02, 9.70707336e-02, 8.84774650e-02, 1.94097092e-02,\n",
      "        7.52840118e-02, 9.40174412e-02],\n",
      "       [3.65880927e-02, 3.60856998e-02, 9.84793544e-02, 2.82194983e-02,\n",
      "        1.27562747e-03, 8.72885326e-03, 5.55987374e-02, 1.89266187e-02,\n",
      "        3.58887529e-02, 9.49374935e-02],\n",
      "       [3.09211340e-03, 3.61197955e-02, 9.63891376e-02, 7.10853260e-02,\n",
      "        3.47256420e-02, 7.36858202e-02, 9.81345257e-02, 8.29901907e-02,\n",
      "        1.03686807e-02, 1.42915594e-02],\n",
      "       [2.29475613e-02, 5.97826617e-02, 4.77846752e-02, 4.70378235e-02,\n",
      "        7.42336461e-02, 8.86557276e-03, 2.26441777e-03, 7.58897269e-02,\n",
      "        5.55332793e-02, 8.41856980e-02],\n",
      "       [9.52705874e-02, 2.01646553e-02, 5.36422250e-02, 6.88291559e-02,\n",
      "        3.18121410e-02, 4.75997513e-02, 2.36383414e-02, 8.12698708e-02,\n",
      "        1.92729578e-02, 6.23387936e-02],\n",
      "       [7.43295663e-02, 6.56440199e-02, 2.72703477e-03, 5.17056647e-02,\n",
      "        1.63764012e-02, 5.64639696e-02, 9.28449575e-02, 7.91611222e-02,\n",
      "        2.72536981e-02, 1.31494066e-02],\n",
      "       [7.00633971e-02, 8.33589015e-02, 2.30318390e-02, 1.53445595e-02,\n",
      "        1.71455370e-02, 4.29902345e-02, 4.12570445e-02, 3.72587366e-02,\n",
      "        1.93126773e-02, 5.80206408e-02],\n",
      "       [1.87782133e-02, 6.11099004e-02, 9.06612994e-02, 2.46165683e-02,\n",
      "        4.71540216e-02, 7.61330556e-02, 5.87671268e-02, 4.91720823e-02,\n",
      "        7.03995849e-02, 2.76010890e-02],\n",
      "       [1.87393156e-03, 6.91377336e-02, 3.17295393e-02, 1.15195336e-02,\n",
      "        9.04825476e-02, 6.33674212e-02, 2.76350386e-02, 8.83977324e-02,\n",
      "        9.30940767e-02, 4.36720755e-02],\n",
      "       [6.83330895e-03, 7.72319401e-02, 3.51284002e-02, 7.96882900e-02,\n",
      "        7.17624318e-02, 8.13018489e-02, 4.03242353e-02, 9.22175579e-03,\n",
      "        4.52088473e-02, 9.47180012e-02],\n",
      "       [9.67371314e-02, 2.27551997e-02, 9.55951073e-02, 9.17617754e-02,\n",
      "        8.80276264e-02, 1.89968636e-02, 5.44397068e-02, 7.69290882e-02,\n",
      "        3.55436719e-02, 7.31942076e-02],\n",
      "       [5.96652647e-02, 8.01174438e-02, 6.48589682e-02, 3.23330586e-02,\n",
      "        4.54218496e-02, 1.72256521e-03, 1.69112670e-02, 7.62396879e-02,\n",
      "        6.41355622e-02, 3.18789511e-02],\n",
      "       [4.04902490e-02, 3.37441170e-02, 4.41300831e-02, 6.56670537e-02,\n",
      "        1.73444647e-03, 8.58439910e-02, 7.88983040e-02, 7.14201787e-02,\n",
      "        5.02885393e-02, 3.59331502e-04]])]\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n"
     ]
    }
   ],
   "source": [
    "#ip_neurons = 3 #input layer\n",
    "ip_neurons = 784\n",
    "num_hidden = 2\n",
    "encoding_bits=10\n",
    "sizes = [50,100]\n",
    "sizes.insert(0,ip_neurons)\n",
    "sizes.append(encoding_bits)\n",
    "print(sizes)\n",
    "print(len(sizes))\n",
    "\n",
    "weight_matrix_list = []\n",
    "\n",
    "bias_list =[]\n",
    "y_hat= []\n",
    "\n",
    "for i in range(int(num_hidden)+1):\n",
    "    weight_matrix_list.append(np.random.rand(sizes[i],sizes[i+1])/np.sqrt(sizes[i]))\n",
    "    bias_list.append(0*np.random.rand(sizes[i+1],1))\n",
    "print(weight_matrix_list)\n",
    "#print(len(weight_matrix_list))\n",
    "\n",
    "print(bias_list)\n",
    "#print(hlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "    \n",
    "    len_w = len(weight_matrix_list)\n",
    "    len_b = len(bias_list)\n",
    "    data_length = len(xtrain)\n",
    "    batch_size = 10\n",
    "    eta=0.01\n",
    "    no_of_batch = int(data_length/batch_size)\n",
    "    epochs =30\n",
    "    steps = 0\n",
    "    loss=0\n",
    "    error = 0\n",
    "    nos_points = 0\n",
    "    file = open('log_file.txt', 'w')\n",
    "    for t in range(epochs):\n",
    "        loss=0\n",
    "        error=0\n",
    "        for batch in range(no_of_batch):\n",
    "            dw=[]\n",
    "            db=[]\n",
    "            for i in range(int(num_hidden)+1):\n",
    "                dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "                db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "            for data_index_in_batch in range(batch_size):\n",
    "                l=batch_size*batch + data_index_in_batch\n",
    "                r=batch_size*batch + data_index_in_batch + 1\n",
    "#                 print(\"before\\n\",data_index_in_batch)\n",
    "                feedforward(xtrain[l:r,:],num_hidden)\n",
    "#                 print(\"after\\n\",y_hat)\n",
    "#                 print(l , r)\n",
    "                backpropagation(y_true[:,l:r])\n",
    "\n",
    "                for j in range(len_w):\n",
    "                    dw[j] += grad_w_k_list[j].transpose()\n",
    "\n",
    "                for j in range(len_b):\n",
    "                    db[j] += grad_b[j]\n",
    "                \n",
    "                \n",
    "                loss += cal_loss(y_hat,y_true[:,l:r])\n",
    "                \n",
    "                error += cal_error(y_hat,y_true[:,l:r])\n",
    "                \n",
    "                nos_points +=1\n",
    "                \n",
    "            for j in range(len_w):\n",
    "                weight_matrix_list[j] = weight_matrix_list[j] - eta*dw[j]\n",
    "\n",
    "            for j in range(len_b):\n",
    "                bias_list[j] =  bias_list[j] - eta*db[j]\n",
    "            \n",
    "            if (steps%300==0):\n",
    "                file.write(\"Epoch \"+str(t+1)+\", Step \"+str(steps)+\", loss : \"+str(loss)+\", error : \"+str(error/nos_points)+\", lr: \" + str(eta)+ \"\\n\")\n",
    "#                 print(\"loss \\n\",loss)\n",
    "            steps += 1\n",
    "            \n",
    "            \n",
    "#             print(\"after batch \", batch)\n",
    "#             print(\"y_hat\", y_hat)\n",
    "            \n",
    "#             print(\"subtracted weight\\n\", weight_matrix_list)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"end  epochs :  \", t, loss)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(y_hat,y_true):\n",
    "    loss = 0\n",
    "    for i in range(10):\n",
    "        if(y_hat[i]!=0):\n",
    "            loss += (-1)*y_true[i]*math.log(y_hat[i]) \n",
    "    \n",
    "    return loss\n",
    "        \n",
    "def cal_error(y_hat,y_true):\n",
    "    true = classifcation(y_true)\n",
    "    predicted = classifcation(decode_yhat_to_classes(y_hat))\n",
    "    if (true != predicted):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end  epochs :   0 [85738.54342296]\n",
      "end  epochs :   1 [30845.12313987]\n",
      "end  epochs :   2 [21132.8001389]\n",
      "end  epochs :   3 [15954.9657875]\n",
      "end  epochs :   4 [12287.05346344]\n",
      "end  epochs :   5 [9576.26599874]\n",
      "end  epochs :   6 [7443.87721191]\n",
      "end  epochs :   7 [5891.19232748]\n",
      "end  epochs :   8 [4702.44427428]\n",
      "end  epochs :   9 [3754.8230568]\n",
      "end  epochs :   10 [3020.75943143]\n",
      "end  epochs :   11 [2412.91866696]\n",
      "end  epochs :   12 [1888.71289096]\n",
      "end  epochs :   13 [1496.76798849]\n",
      "end  epochs :   14 [1227.93940742]\n",
      "end  epochs :   15 [1050.52608523]\n",
      "end  epochs :   16 [920.22328146]\n",
      "end  epochs :   17 [811.95643289]\n",
      "end  epochs :   18 [717.01776418]\n",
      "end  epochs :   19 [652.68604795]\n",
      "end  epochs :   20 [587.88960546]\n",
      "end  epochs :   21 [536.45091292]\n",
      "end  epochs :   22 [490.61476965]\n",
      "end  epochs :   23 [454.75237784]\n",
      "end  epochs :   24 [421.2531417]\n",
      "end  epochs :   25 [393.10175443]\n",
      "end  epochs :   26 [366.42889894]\n",
      "end  epochs :   27 [340.99526414]\n",
      "end  epochs :   28 [312.48752092]\n",
      "end  epochs :   29 [291.82035953]\n"
     ]
    }
   ],
   "source": [
    "grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes)\n",
    "#print(y_hat)0\n",
    "#print(len(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.75932665e-14 3.90802496e-14 9.74416644e-10 ... 1.67696490e-12\n",
      "  6.92030715e-20 5.91251113e-20]\n",
      " [1.41370952e-06 1.17046026e-13 9.99999788e-01 ... 9.95695069e-10\n",
      "  5.98762419e-14 2.11174747e-16]\n",
      " [1.79138974e-16 5.63427164e-06 4.49225672e-10 ... 7.10184244e-05\n",
      "  2.82842226e-09 1.50243261e-11]\n",
      " ...\n",
      " [2.97640676e-11 1.29621418e-10 3.82646144e-10 ... 1.77283659e-08\n",
      "  2.56000661e-19 5.79414405e-11]\n",
      " [8.47558760e-15 3.95143768e-11 6.66210328e-19 ... 6.16514117e-10\n",
      "  3.19123190e-17 8.68294014e-07]\n",
      " [1.40447280e-10 5.94802479e-04 2.10099484e-07 ... 3.23330801e-08\n",
      "  9.99999965e-01 1.31642835e-11]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feedforward(xtrain,num_hidden)\n",
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 1 ... 6 9 6]\n",
      "[[4]\n",
      " [6]\n",
      " [1]\n",
      " ...\n",
      " [6]\n",
      " [9]\n",
      " [6]]\n",
      "[[4.]\n",
      " [6.]\n",
      " [1.]\n",
      " ...\n",
      " [6.]\n",
      " [9.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)\n",
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993454545454545\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in range(55000):\n",
    "    if(y_predict[i]==true_label[i]):\n",
    "        cnt+=1\n",
    "acc = cnt/55000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('valid.csv').as_matrix()\n",
    "xtrain=valid[:,1:785]\n",
    "\n",
    "xmean = np.mean(xtrain , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "xtrain = xtrain - means_expanded\n",
    "\n",
    "xstd = np.std(xtrain , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "xtrain = xtrain*1.0/std_expanded \n",
    "\n",
    "true_label = valid[:,785:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.93834554e-11 1.02468803e-17 1.78979642e-16 ... 9.86281578e-01\n",
      "  1.58641175e-11 1.85552545e-06]\n",
      " [9.89884136e-01 3.48945317e-12 5.74066579e-09 ... 2.35231489e-07\n",
      "  6.59855136e-12 1.38623127e-09]\n",
      " [1.35031331e-05 1.00571421e-11 5.87773320e-14 ... 5.44747967e-06\n",
      "  1.27070025e-09 9.99973733e-01]\n",
      " ...\n",
      " [6.17277149e-08 2.65297674e-13 2.45942419e-11 ... 2.84401162e-12\n",
      "  5.03711585e-09 4.47545802e-09]\n",
      " [5.80139558e-08 2.21156031e-18 4.63412291e-10 ... 2.15909371e-11\n",
      "  2.57953402e-02 1.13254908e-07]\n",
      " [1.29953666e-03 9.99999621e-01 1.39413547e-10 ... 7.77268318e-12\n",
      "  3.53749779e-15 6.79707154e-08]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feedforward(xtrain,num_hidden)\n",
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 9 4 ... 0 6 2]\n",
      "[[1]\n",
      " [9]\n",
      " [4]\n",
      " ...\n",
      " [0]\n",
      " [6]\n",
      " [2]]\n",
      "[[1.]\n",
      " [9.]\n",
      " [4.]\n",
      " ...\n",
      " [0.]\n",
      " [6.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)\n",
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "k = len(true_label)\n",
    "for i in range(k):\n",
    "    if(y_predict[i]==true_label[i]):\n",
    "        cnt+=1\n",
    "acc = cnt/k\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "xtest = pd.read_csv('test.csv').as_matrix()\n",
    "test=xtest[:,1:]\n",
    "\n",
    "xmean = np.mean(test , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "test = test - means_expanded\n",
    "\n",
    "xstd = np.std(test , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "test = test*1.0/std_expanded \n",
    "\n",
    "feedforward(test,num_hidden)\n",
    "#print(y_hat)\n",
    "# y_hat = decode_yhat_to_classes(y_hat)\n",
    "#print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.53762045e-10 2.77618754e-11 1.05436853e-12 ... 2.44743015e-10\n",
      "  2.60388112e-06 1.82295881e-12]\n",
      " [7.73721477e-11 3.00693989e-11 5.38004485e-07 ... 6.65360084e-07\n",
      "  1.58629553e-07 9.99999999e-01]\n",
      " [9.86044509e-01 7.58832220e-05 2.68404477e-04 ... 8.63218592e-05\n",
      "  3.88119656e-02 4.18564324e-17]\n",
      " ...\n",
      " [2.14408314e-06 1.26038164e-06 5.34429980e-11 ... 4.80418072e-14\n",
      "  4.26157543e-05 1.62400035e-11]\n",
      " [7.41838853e-03 2.72672255e-01 7.86506236e-06 ... 1.29930392e-15\n",
      "  9.61050160e-01 6.56175854e-22]\n",
      " [5.08682570e-03 9.60863305e-08 9.98863424e-01 ... 9.90874369e-01\n",
      "  2.27727935e-08 4.49782196e-11]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 6 9 ... 9 8 1]\n",
      "[[2]\n",
      " [6]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [8]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_to_y_predict(y_predict):\n",
    "    ide=[]\n",
    "    row=len(y_predict)\n",
    "    for i in range(row):\n",
    "        ide.append(i)\n",
    "    ide=np.array(ide)\n",
    "    ide=ide[:,np.newaxis]\n",
    "    y= np.hstack((ide,y_predict))\n",
    "    #print(y_predict, ide)\n",
    "    return y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [6]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [8]\n",
      " [1]]\n",
      "[[   0    2]\n",
      " [   1    6]\n",
      " [   2    9]\n",
      " ...\n",
      " [9997    9]\n",
      " [9998    8]\n",
      " [9999    1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)\n",
    "#y_predict= np.random.randint(10, size=(10000, 1))\n",
    "y_predict= add_id_to_y_predict(y_predict)\n",
    "print(y_predict)\n",
    "x,y = y_predict.shape\n",
    "#print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_test_today.csv', 'w', newline='') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    row,col=y_predict.shape \n",
    "    thewriter.writerow(['id','label'])\n",
    "    for i in range(row):\n",
    "        thewriter.writerow(y_predict[i])\n",
    "#out.close()\n",
    "#np.savetxt(\"foo.csv\", y_predict, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
