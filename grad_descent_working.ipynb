{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import openpyxl\n",
    "import math\n",
    "'''train = np.array([\n",
    "    [0,0,1,2,3],\n",
    "    [1,1,2,3,4],\n",
    "    [2,2,3,4,5],\n",
    "    [3,3,4,5,6]\n",
    "]\n",
    ")'''\n",
    "'''\n",
    "train = np.array([\n",
    "    [0,1,1,1,3],\n",
    "    [1,1,1,1,4],\n",
    "    [2,1,1,1,5],\n",
    "    [3,1,1,1,6]\n",
    "]\n",
    ")'''\n",
    "train = pd.read_csv('train.csv').as_matrix()#read train.csv file\n",
    "#print(type(train))\n",
    "#xtrain=train[:,1:4]\n",
    "#xtrain = xtrain/5\n",
    "#true_label=train[:,4:5]\n",
    "#print((xtrain),(true_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_into_prob(x,max_val):\n",
    "    len=x.size\n",
    "    y_list=[]\n",
    "    for i in range(len):\n",
    "        temp=[]\n",
    "        for j in range(max_val):\n",
    "            if(x[i]==j):\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "            #print(temp)\n",
    "        y_list.append(temp)\n",
    "    return y_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.55945675 -0.55945675  2.06254185 ...  2.0522595  -0.55945675\n",
      "  -0.55945675]\n",
      " [-0.5380259  -0.5380259  -0.5380259  ... -0.5380259  -0.5380259\n",
      "  -0.5380259 ]\n",
      " [-0.6486501  -0.6486501  -0.6486501  ... -0.6486501  -0.6486501\n",
      "  -0.6486501 ]\n",
      " ...\n",
      " [-0.64973918 -0.64973918 -0.64973918 ... -0.64973918 -0.64973918\n",
      "  -0.64973918]\n",
      " [-0.59745866 -0.59745866 -0.59745866 ... -0.59745866 -0.59745866\n",
      "  -0.59745866]\n",
      " [-0.62629522 -0.62629522 -0.62629522 ... -0.62629522 -0.62629522\n",
      "  -0.62629522]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "xtrain=train[:,1:785]\n",
    "\n",
    "xmean = np.mean(xtrain , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "xtrain = xtrain - means_expanded\n",
    "\n",
    "xstd = np.std(xtrain , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "xtrain = xtrain*1.0/std_expanded \n",
    "\n",
    "true_label = train[:,785:]\n",
    "y_true=np.array(encode_into_prob(true_label,10))\n",
    "y_true=y_true.transpose()\n",
    "\n",
    "#print(type(y_true))\n",
    "#print(y_true.transpose())\n",
    "print(xtrain)\n",
    "print((y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(batch_train,hidden_layers):\n",
    "     \n",
    "    global weight_matrix_list\n",
    "    global bias_list\n",
    "    global preactivation_list \n",
    "    preactivation_list=[]\n",
    "    global activation_list\n",
    "    activation_list=[]\n",
    "    global y_hat\n",
    "    y_hat=[]\n",
    "    a_new=[]\n",
    "    a_last_layer=[]\n",
    "    nos_data = len(batch_train)\n",
    "    #print(nos_data)\n",
    "    h=batch_train.transpose()\n",
    "    preactivation_list.append(h)\n",
    "    activation_list.append(h)\n",
    "    #print(h)\n",
    "    \n",
    "#     print(\"y_hat in feed \\n\",y_hat )\n",
    "    \n",
    "    for j in range(hidden_layers):\n",
    "        \n",
    "#         print(\".............................\")\n",
    "#         print(\"activation for layer \", j)\n",
    "#         print(activation_list[j] , activation_list[j].shape)\n",
    "#         print(\"weight\",j+1)\n",
    "#         print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#         print(\"bias \",j+1)\n",
    "#         print(bias_list[j],bias_list[j].shape)\n",
    "        \n",
    "        a_new = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "        preactivation_list.append(a_new)\n",
    "        \n",
    "#         print(\"preactivation for layer\",j+1)\n",
    "#         print(preactivation_list[j+1], preactivation_list[j+1].shape)\n",
    "        \n",
    "        h = 1.0/(1.0+np.exp(-1*preactivation_list[j+1]))\n",
    "        activation_list.append(h)\n",
    "\n",
    "    \n",
    "    j=hidden_layers\n",
    "#     print(\".............................\")\n",
    "#     print(\"activation for layer \", j)\n",
    "#     print(activation_list[j] , activation_list[j].shape)\n",
    "#     print(\"weight\",j+1)\n",
    "#     print(weight_matrix_list[j], weight_matrix_list[j].shape)\n",
    "#     print(\"bias \",j+1)\n",
    "#     print(bias_list[j],bias_list[j].shape)\n",
    "    \n",
    "    a_last_layer = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),activation_list[j])\n",
    "    preactivation_list.append(a_last_layer)\n",
    "    \n",
    "#     print(\"last activation\")\n",
    "#     print(a_last_layer,a_last_layer.shape)\n",
    "\n",
    "    y_hat = np.array(output(a_last_layer,batch_train))\n",
    "    y_hat = y_hat.transpose()\n",
    "    \n",
    "#     print(\"y_hat in feedforward end\")\n",
    "#     print(y_hat)\n",
    "    \n",
    "    \n",
    "def output(a_last_layer,batch_train):\n",
    "    y_hat_initial=[]\n",
    "    y_hat_local=[]\n",
    "    nos_data=len(batch_train)\n",
    "    for i in range(nos_data):\n",
    "        y_hat_initial.append(softmax(a_last_layer[:,i:i+1].transpose()))\n",
    "\n",
    "    length = len(y_hat_initial)\n",
    "    for i in range(length):\n",
    "        y_hat_local.append(y_hat_initial[i][0])\n",
    "    \n",
    "#     print(\"y_hat_local\", y_hat_local)\n",
    "#     print(\"y_hat_initial\",y_hat_initial)\n",
    "    return y_hat_local\n",
    "\n",
    "def softmax(row_vector):\n",
    "    sum_all=0\n",
    "    row_vector = np.exp(row_vector)\n",
    "    sum_all = np.sum(row_vector , axis=1)         #print(\"row_vector\", row_vector)\n",
    "    row_vector = (row_vector*1.0/sum_all)         #print(\"sum_all\",sum_all)\n",
    "    return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(y_true_batch):\n",
    "    \n",
    "#     print(\"weigt in backpropogation\", weight_matrix_list)\n",
    "#     print(\"...........y_true_batch..........\\n\",y_true_batch)\n",
    "    \n",
    "    global grad_a_k_list\n",
    "    grad_a_k_list=[]\n",
    "    \n",
    "    global grad_w_k_list\n",
    "    grad_w_k_list=[]\n",
    "    \n",
    "    global grad_h_k_list\n",
    "    grad_h_k_list=[]\n",
    "    \n",
    "    global grad_b_k_list\n",
    "    grad_b_k_list=[]\n",
    "    \n",
    "    global grad_b\n",
    "    grad_b=[]\n",
    "    \n",
    "    global grad_g_dash_a_k_list\n",
    "    grad_g_dash_a_k_list = []\n",
    "    \n",
    "    global grad_a_last\n",
    "    grad_a_last=[]\n",
    "    \n",
    "    k = num_hidden+1\n",
    "    \n",
    "#     print(\"--------------------------in Backpropagation---------------------\")\n",
    "    \n",
    "    grad_g_dash_a_k_list = g_dash(preactivation_list)\n",
    "    \n",
    "#     print(\"preactivation_list....................\")\n",
    "#     print(\"Nos. of matrices in preactivation_list is\", len(preactivation_list))\n",
    "#     print(preactivation_list)\n",
    "#     print(\"g_dash..................\")\n",
    "#     print(\"Nos. of matrices in g_dash is \", len(grad_g_dash_a_k_list))\n",
    "#     print(grad_g_dash_a_k_list)\n",
    "    \n",
    "    grad_a_last = -np.subtract(y_true_batch,y_hat)\n",
    "    \n",
    "#     print(\"y_true_batch \\n\", y_true_batch)\n",
    "#     print(\"y_hat \\n\",y_hat)\n",
    "    \n",
    "    grad_a_k_list.insert(0,grad_a_last)\n",
    "    \n",
    "#     print(\"grad_a_last_layer\", k)\n",
    "#     print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "  \n",
    "    \n",
    "    while(k>0):\n",
    "        \n",
    "#         print(\".................grad_compute at layer\",k)\n",
    "        \n",
    "        grad_w_k_list.insert(0,np.matmul(grad_a_k_list[0],activation_list[k-1].transpose()))\n",
    "        \n",
    "#         print(\"grad_a\", k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print('preactivation  a_',k-1)\n",
    "#         print(preactivation_list[k-1],preactivation_list[k-1].shape)\n",
    "#         print(\"grad_w_\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "        \n",
    "        grad_b_k_list.insert(0,grad_a_k_list[0])\n",
    "        \n",
    "#         print(\"grad_b\",k)\n",
    "#         print(grad_b_k_list[0],grad_b_k_list[0].shape)\n",
    "        \n",
    "        grad_h_k_list.insert(0,np.matmul(weight_matrix_list[k-1],grad_a_k_list[0]))\n",
    "        \n",
    "#         print(\"grad_w\",k)\n",
    "#         print(grad_w_k_list[0],grad_w_k_list[0].shape)\n",
    "#         print(\"grad_a\",k)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "#         print(\"grad_h\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "        \n",
    "        grad_a_k_list.insert(0,np.multiply(grad_h_k_list[0],grad_g_dash_a_k_list[k-1]))\n",
    "        \n",
    "#         print(\"grad_h_k_list[0]\",k-1)\n",
    "#         print(grad_h_k_list[0],grad_h_k_list[0].shape)\n",
    "#         print(\"grad_g_dash_a_k_list[k-1]\",k-1)\n",
    "#         print(grad_g_dash_a_k_list[k-1],grad_g_dash_a_k_list[k-1].shape)\n",
    "#         print(\"grad_a\",k-1)\n",
    "#         print(grad_a_k_list[0],grad_a_k_list[0].shape)\n",
    "        k=k-1\n",
    "#         print(\"grad_a_ik_list\",grad_a_k_list[0])\n",
    "    \n",
    "    sum_all_grad_b_of_data(grad_b_k_list)\n",
    "    \n",
    "\n",
    "def sum_all_grad_b_of_data(grad_b_k_list):\n",
    "    length=len(grad_b_k_list)\n",
    "    i=0\n",
    "    while(i<length):\n",
    "        row=[]\n",
    "        row = np.sum(grad_b_k_list[i],1)\n",
    "        j = len(row)\n",
    "        #print(row.shape)\n",
    "        row = row[:, np.newaxis]\n",
    "        #print(row.shape , row)\n",
    "        grad_b.append(row)\n",
    "        i+=1\n",
    "    #print(\"grad_b\",grad_b)\n",
    "\n",
    "def g_dash(preactivation_list):\n",
    "    length=len(preactivation_list)\n",
    "    a_modified = preactivation_list[0:length]\n",
    "    g_dash_a_k_list=[]\n",
    "    for i in range(length):\n",
    "        a=(1.0/(1+np.exp(-a_modified[i])))*(1-(1.0/(1+np.exp(-a_modified[i]))))\n",
    "        g_dash_a_k_list.append(a)\n",
    "    return g_dash_a_k_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_yhat_to_classes(y_hat_local):\n",
    "    col = len(y_hat_local[0])\n",
    "    max_val=0\n",
    "    for j in range(col):\n",
    "        max_val = y_hat_local[0][j]\n",
    "        index=0\n",
    "        for i in range(9):\n",
    "            if(y_hat_local[i+1][j] > max_val):\n",
    "                y_hat_local[index][j]=0\n",
    "                index = i+1\n",
    "                max_val=y_hat[i+1][j]\n",
    "            else :\n",
    "                y_hat_local[i+1][j]=0\n",
    "        y_hat_local[index][j]=1\n",
    "    return y_hat_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i+1)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 50, 100, 10]\n",
      "4\n",
      "[array([[0.00109635, 0.03219259, 0.01074146, ..., 0.00643851, 0.02192284,\n",
      "        0.03315315],\n",
      "       [0.00630526, 0.00129497, 0.0117237 , ..., 0.03284712, 0.01340493,\n",
      "        0.01280288],\n",
      "       [0.01287493, 0.01692015, 0.02666916, ..., 0.01502726, 0.03501396,\n",
      "        0.00380961],\n",
      "       ...,\n",
      "       [0.01356819, 0.03011855, 0.03057457, ..., 0.02199732, 0.03455368,\n",
      "        0.01514046],\n",
      "       [0.01647585, 0.02579763, 0.01382885, ..., 0.00979694, 0.02012946,\n",
      "        0.03198862],\n",
      "       [0.00777396, 0.008059  , 0.02972664, ..., 0.01440984, 0.00853415,\n",
      "        0.0058198 ]]), array([[0.09876103, 0.03416655, 0.01274727, ..., 0.11155943, 0.11013763,\n",
      "        0.01382684],\n",
      "       [0.04834662, 0.06988934, 0.12951638, ..., 0.09927345, 0.12455441,\n",
      "        0.07751043],\n",
      "       [0.14055496, 0.10900656, 0.00912434, ..., 0.00400206, 0.01454718,\n",
      "        0.09377716],\n",
      "       ...,\n",
      "       [0.115821  , 0.02716956, 0.0829549 , ..., 0.03346369, 0.13894221,\n",
      "        0.14034268],\n",
      "       [0.0094702 , 0.1221877 , 0.02015952, ..., 0.06063676, 0.03367125,\n",
      "        0.10340358],\n",
      "       [0.09325179, 0.06299533, 0.12079515, ..., 0.12362356, 0.03251121,\n",
      "        0.02575601]]), array([[0.04245029, 0.05130374, 0.0968679 , 0.07321498, 0.02154135,\n",
      "        0.0158578 , 0.08718455, 0.01423281, 0.02363793, 0.09639578],\n",
      "       [0.01556921, 0.00503358, 0.090226  , 0.07559506, 0.00764645,\n",
      "        0.06726448, 0.01112592, 0.07019994, 0.09485287, 0.05378597],\n",
      "       [0.00184936, 0.03621434, 0.02753806, 0.05975169, 0.05396232,\n",
      "        0.04145838, 0.01015092, 0.01936888, 0.05560925, 0.08323771],\n",
      "       [0.07564165, 0.09919637, 0.09658845, 0.08815032, 0.01523983,\n",
      "        0.00069526, 0.01666402, 0.07624015, 0.00193282, 0.05800865],\n",
      "       [0.07000545, 0.08694014, 0.05989197, 0.03178672, 0.07282022,\n",
      "        0.03040418, 0.06852519, 0.01092275, 0.07781636, 0.07685599],\n",
      "       [0.06483389, 0.04398246, 0.02148157, 0.04748137, 0.06628726,\n",
      "        0.07434787, 0.03281465, 0.07476064, 0.02361863, 0.06756527],\n",
      "       [0.07901547, 0.04795976, 0.05622656, 0.08574714, 0.0693221 ,\n",
      "        0.07668945, 0.08385599, 0.07012427, 0.06087654, 0.0695145 ],\n",
      "       [0.04462584, 0.07817088, 0.03641703, 0.03359419, 0.0417154 ,\n",
      "        0.03043788, 0.03992483, 0.05788568, 0.03531282, 0.03193934],\n",
      "       [0.04259971, 0.04491018, 0.0469215 , 0.00178597, 0.0348077 ,\n",
      "        0.01012976, 0.08884111, 0.05918134, 0.02248673, 0.00499328],\n",
      "       [0.04123562, 0.07516838, 0.07778644, 0.00469864, 0.07586454,\n",
      "        0.04942074, 0.09144929, 0.09668856, 0.05587984, 0.08572369],\n",
      "       [0.08267179, 0.08655953, 0.07950143, 0.01698282, 0.04068275,\n",
      "        0.07897158, 0.05479888, 0.01783429, 0.05622804, 0.07982677],\n",
      "       [0.08517779, 0.04469475, 0.03957406, 0.09632123, 0.01242857,\n",
      "        0.07710089, 0.08685991, 0.08036156, 0.08403239, 0.00188818],\n",
      "       [0.01748119, 0.08234368, 0.0346873 , 0.02261568, 0.02369794,\n",
      "        0.03228772, 0.07890352, 0.0127177 , 0.03657958, 0.02975823],\n",
      "       [0.01352142, 0.08291005, 0.01690447, 0.09831803, 0.00197883,\n",
      "        0.00821009, 0.06571873, 0.06381002, 0.01966151, 0.0083794 ],\n",
      "       [0.05899912, 0.04248277, 0.08839205, 0.0627351 , 0.05751582,\n",
      "        0.04459702, 0.0491682 , 0.02368095, 0.09000909, 0.00068821],\n",
      "       [0.01018589, 0.06238013, 0.02586902, 0.09101348, 0.06572391,\n",
      "        0.04148485, 0.06274925, 0.00848951, 0.0108877 , 0.02094593],\n",
      "       [0.09161933, 0.01309558, 0.02113618, 0.05577338, 0.09614701,\n",
      "        0.08683729, 0.02294149, 0.09056874, 0.08278946, 0.01391171],\n",
      "       [0.0667044 , 0.08911576, 0.08885101, 0.01231826, 0.00859193,\n",
      "        0.03645268, 0.03272353, 0.08455755, 0.00614798, 0.0607934 ],\n",
      "       [0.09075264, 0.02242295, 0.08234432, 0.02935689, 0.02685923,\n",
      "        0.03995115, 0.07076509, 0.02424097, 0.06050074, 0.04732244],\n",
      "       [0.04228136, 0.08088094, 0.08336126, 0.0684608 , 0.09293083,\n",
      "        0.01451703, 0.09235395, 0.01949512, 0.04258776, 0.05792901],\n",
      "       [0.00131342, 0.02807356, 0.08303672, 0.09231032, 0.03577379,\n",
      "        0.0012355 , 0.04633781, 0.05339412, 0.02706867, 0.05126195],\n",
      "       [0.01708529, 0.09982892, 0.06638827, 0.08765332, 0.09437637,\n",
      "        0.0516959 , 0.02498006, 0.01236291, 0.09883278, 0.0237819 ],\n",
      "       [0.0830357 , 0.00190081, 0.09225123, 0.05176247, 0.05395264,\n",
      "        0.0186831 , 0.00517593, 0.08049616, 0.09619406, 0.02707675],\n",
      "       [0.01308039, 0.06282437, 0.08304047, 0.00700502, 0.06829834,\n",
      "        0.01987683, 0.07315428, 0.02713794, 0.06891958, 0.08696153],\n",
      "       [0.07117989, 0.07218495, 0.06201182, 0.03865378, 0.03322373,\n",
      "        0.01764298, 0.01731061, 0.04974933, 0.04204533, 0.06626622],\n",
      "       [0.02047514, 0.04867099, 0.04449514, 0.03686275, 0.05993075,\n",
      "        0.06127734, 0.03913385, 0.02869318, 0.08277382, 0.02161595],\n",
      "       [0.07386411, 0.04072893, 0.00520866, 0.03524201, 0.02383088,\n",
      "        0.01925732, 0.06359203, 0.03872499, 0.02921785, 0.08473668],\n",
      "       [0.06773523, 0.05818489, 0.01394989, 0.05632491, 0.09107166,\n",
      "        0.01576383, 0.04321835, 0.08955804, 0.05931481, 0.05452492],\n",
      "       [0.08821268, 0.07245163, 0.05575065, 0.09891021, 0.0349697 ,\n",
      "        0.0761486 , 0.06462576, 0.03705653, 0.03283884, 0.09700483],\n",
      "       [0.00986149, 0.09498342, 0.05871892, 0.04863064, 0.0274147 ,\n",
      "        0.09998764, 0.01697277, 0.06781032, 0.01873887, 0.02573606],\n",
      "       [0.01022503, 0.0204888 , 0.07783701, 0.05308205, 0.04887052,\n",
      "        0.00574261, 0.08442729, 0.03145655, 0.07643769, 0.0608555 ],\n",
      "       [0.05515895, 0.08130277, 0.09784559, 0.03736994, 0.03800472,\n",
      "        0.07488221, 0.02021848, 0.06863797, 0.06883772, 0.07154454],\n",
      "       [0.01470812, 0.06015301, 0.08689309, 0.02933904, 0.0658637 ,\n",
      "        0.07175243, 0.059549  , 0.07154546, 0.00031857, 0.06712572],\n",
      "       [0.07248309, 0.08202449, 0.08916348, 0.05945649, 0.05121116,\n",
      "        0.02101057, 0.04377282, 0.09696415, 0.02236492, 0.02658735],\n",
      "       [0.07392587, 0.06362219, 0.04541328, 0.08979724, 0.05652085,\n",
      "        0.01706672, 0.02728201, 0.00577636, 0.0898553 , 0.09127361],\n",
      "       [0.06038784, 0.09368626, 0.0040482 , 0.02672198, 0.08961379,\n",
      "        0.06722802, 0.08644363, 0.08635155, 0.01756845, 0.02673737],\n",
      "       [0.07160553, 0.06300082, 0.03678974, 0.0016545 , 0.08386071,\n",
      "        0.02594926, 0.06197422, 0.08544848, 0.01952338, 0.05595292],\n",
      "       [0.01527658, 0.01738717, 0.02235087, 0.03699661, 0.00792119,\n",
      "        0.0285474 , 0.06241176, 0.03435854, 0.0791608 , 0.06153154],\n",
      "       [0.02535373, 0.05422615, 0.03348823, 0.07553431, 0.07117159,\n",
      "        0.05443705, 0.09534376, 0.0367508 , 0.05757773, 0.04669452],\n",
      "       [0.03690291, 0.07940969, 0.03471611, 0.07587749, 0.0111185 ,\n",
      "        0.06806224, 0.0604372 , 0.06785371, 0.00492347, 0.07910284],\n",
      "       [0.07314347, 0.01957957, 0.01724577, 0.00063277, 0.04375348,\n",
      "        0.06637195, 0.08648107, 0.0146283 , 0.05692055, 0.0639017 ],\n",
      "       [0.0991554 , 0.04408333, 0.08622751, 0.08598715, 0.06182782,\n",
      "        0.03929731, 0.03548251, 0.05429337, 0.00257799, 0.04579434],\n",
      "       [0.06495248, 0.03528387, 0.0511041 , 0.02534301, 0.05678604,\n",
      "        0.05730495, 0.07413266, 0.05039781, 0.05715753, 0.0915811 ],\n",
      "       [0.07903207, 0.01140749, 0.02814141, 0.00173261, 0.06350619,\n",
      "        0.06880145, 0.0878471 , 0.03744738, 0.0193042 , 0.034229  ],\n",
      "       [0.07101196, 0.07317851, 0.0617728 , 0.06130676, 0.08806026,\n",
      "        0.05142995, 0.02885845, 0.00485056, 0.05798278, 0.07506867],\n",
      "       [0.05672615, 0.06236769, 0.08772118, 0.08229653, 0.04713111,\n",
      "        0.06816445, 0.06030506, 0.07140553, 0.02453391, 0.04420654],\n",
      "       [0.04240278, 0.08433512, 0.03167288, 0.05234903, 0.03285831,\n",
      "        0.07608509, 0.08099153, 0.09863087, 0.03111877, 0.09320471],\n",
      "       [0.01706884, 0.04447362, 0.00477235, 0.05065338, 0.05981339,\n",
      "        0.05396383, 0.03412886, 0.05575976, 0.05953581, 0.04479605],\n",
      "       [0.04485946, 0.09904245, 0.08067726, 0.0622543 , 0.02349137,\n",
      "        0.03294383, 0.07066407, 0.0901845 , 0.07849742, 0.03686746],\n",
      "       [0.03806147, 0.09066894, 0.0884895 , 0.09322502, 0.05756792,\n",
      "        0.05821823, 0.05936718, 0.0429251 , 0.03265637, 0.09891335],\n",
      "       [0.00034088, 0.0365672 , 0.00949844, 0.00849661, 0.08316624,\n",
      "        0.05830103, 0.05184971, 0.02346545, 0.07428653, 0.0596043 ],\n",
      "       [0.0654937 , 0.0160796 , 0.05051777, 0.06975916, 0.01320383,\n",
      "        0.06288727, 0.07209426, 0.07818701, 0.09835384, 0.08507432],\n",
      "       [0.08591687, 0.03295625, 0.01946778, 0.08757721, 0.05728851,\n",
      "        0.06211598, 0.04841598, 0.07530995, 0.05589684, 0.04375488],\n",
      "       [0.06971366, 0.04389929, 0.03211864, 0.08568792, 0.06763837,\n",
      "        0.02067384, 0.02372022, 0.02441664, 0.08093097, 0.00879591],\n",
      "       [0.03607131, 0.00984973, 0.09647258, 0.00872121, 0.04263233,\n",
      "        0.0125662 , 0.00569228, 0.04674123, 0.09749088, 0.04917925],\n",
      "       [0.03162501, 0.05668634, 0.01745716, 0.07624175, 0.08269941,\n",
      "        0.0129961 , 0.07977497, 0.06901792, 0.01881488, 0.00840792],\n",
      "       [0.05994409, 0.01101673, 0.0911751 , 0.01487287, 0.08545841,\n",
      "        0.06672696, 0.04906913, 0.09345657, 0.0153577 , 0.06823596],\n",
      "       [0.02077347, 0.02434069, 0.0880001 , 0.07509513, 0.00553398,\n",
      "        0.08473648, 0.0610617 , 0.0493266 , 0.07714323, 0.03781222],\n",
      "       [0.01803722, 0.03086148, 0.09055749, 0.07339149, 0.0922615 ,\n",
      "        0.06621913, 0.06124956, 0.00371107, 0.08880102, 0.01644967],\n",
      "       [0.05569486, 0.04070217, 0.07006196, 0.08848526, 0.01436658,\n",
      "        0.02050554, 0.06309169, 0.05899254, 0.02347228, 0.0246687 ],\n",
      "       [0.04004047, 0.01861677, 0.03308188, 0.09745162, 0.07322581,\n",
      "        0.02282718, 0.01425375, 0.05997602, 0.06138806, 0.06163409],\n",
      "       [0.07453386, 0.0175763 , 0.01769119, 0.09365877, 0.03824698,\n",
      "        0.00999184, 0.04272492, 0.04317068, 0.04462896, 0.09656161],\n",
      "       [0.08965875, 0.00131273, 0.00164468, 0.00612487, 0.00264584,\n",
      "        0.04981571, 0.0949395 , 0.00066715, 0.07948365, 0.00357663],\n",
      "       [0.02968073, 0.00858922, 0.03951624, 0.08576836, 0.04098584,\n",
      "        0.09861762, 0.07402787, 0.06309039, 0.01979544, 0.0983689 ],\n",
      "       [0.02435138, 0.03015396, 0.0028316 , 0.07023636, 0.02450159,\n",
      "        0.03483394, 0.06879508, 0.07977713, 0.01197159, 0.05019183],\n",
      "       [0.02723753, 0.09535155, 0.05212425, 0.02392749, 0.04003183,\n",
      "        0.01547959, 0.06100619, 0.0040083 , 0.07027672, 0.060444  ],\n",
      "       [0.0997928 , 0.06720028, 0.02308691, 0.00426732, 0.00051125,\n",
      "        0.04444866, 0.09334809, 0.03058326, 0.02519775, 0.0030806 ],\n",
      "       [0.06130549, 0.04979388, 0.0640166 , 0.05142662, 0.08164814,\n",
      "        0.07970203, 0.01686386, 0.09246294, 0.02351166, 0.04331299],\n",
      "       [0.05382322, 0.09396339, 0.00359432, 0.00798888, 0.04850122,\n",
      "        0.05071379, 0.03105072, 0.05544521, 0.07789353, 0.04992412],\n",
      "       [0.04786363, 0.05810226, 0.02429008, 0.07527946, 0.08325674,\n",
      "        0.08678331, 0.07859984, 0.01419794, 0.00678598, 0.05107376],\n",
      "       [0.04664333, 0.02243969, 0.06037305, 0.01180439, 0.01297543,\n",
      "        0.08460948, 0.04538396, 0.01937255, 0.03831937, 0.09915778],\n",
      "       [0.0582144 , 0.02215871, 0.03347986, 0.02089472, 0.04323717,\n",
      "        0.04532994, 0.09859212, 0.00859566, 0.09457026, 0.0613505 ],\n",
      "       [0.07637979, 0.05430325, 0.03893696, 0.0271418 , 0.05204029,\n",
      "        0.07606631, 0.0934607 , 0.01891107, 0.06581862, 0.08132009],\n",
      "       [0.04675665, 0.08146042, 0.01508992, 0.06952124, 0.03271457,\n",
      "        0.09134811, 0.03289324, 0.02961541, 0.04323546, 0.02236487],\n",
      "       [0.0101636 , 0.04231308, 0.05648887, 0.01009756, 0.03774098,\n",
      "        0.08284118, 0.06087321, 0.02994227, 0.09244463, 0.04923498],\n",
      "       [0.00870644, 0.07415312, 0.04274391, 0.06777699, 0.05864692,\n",
      "        0.0449158 , 0.04544874, 0.08285316, 0.08548217, 0.09512558],\n",
      "       [0.02088927, 0.02714935, 0.09139794, 0.05820441, 0.03500761,\n",
      "        0.09348346, 0.09594312, 0.02511873, 0.01759691, 0.05974942],\n",
      "       [0.04376639, 0.08410018, 0.04315474, 0.02750238, 0.07361245,\n",
      "        0.02424861, 0.09197926, 0.03578259, 0.03966511, 0.0498175 ],\n",
      "       [0.06669111, 0.06656748, 0.08415679, 0.06878966, 0.01690127,\n",
      "        0.05343412, 0.01013224, 0.04612073, 0.04611599, 0.09557228],\n",
      "       [0.02714965, 0.0388127 , 0.02199059, 0.06684998, 0.08566024,\n",
      "        0.08790239, 0.0524687 , 0.04059331, 0.00058575, 0.08189203],\n",
      "       [0.03273972, 0.09972927, 0.01232853, 0.08904349, 0.09999508,\n",
      "        0.03904756, 0.06825356, 0.05027958, 0.07636875, 0.07191842],\n",
      "       [0.04364703, 0.01854814, 0.01766568, 0.04532359, 0.06971343,\n",
      "        0.08220417, 0.04549477, 0.06187117, 0.00790928, 0.02942723],\n",
      "       [0.01578829, 0.00165537, 0.05562156, 0.05995854, 0.09777909,\n",
      "        0.0644128 , 0.07406177, 0.02449353, 0.00257733, 0.09614336],\n",
      "       [0.03223537, 0.0447439 , 0.09459789, 0.04270442, 0.01649038,\n",
      "        0.0030318 , 0.0391181 , 0.0478301 , 0.07870666, 0.03771389],\n",
      "       [0.00310717, 0.09101898, 0.05267   , 0.0165195 , 0.09995281,\n",
      "        0.03116296, 0.0883569 , 0.09842048, 0.05055153, 0.0747081 ],\n",
      "       [0.09603104, 0.02339563, 0.04913253, 0.09631933, 0.01407595,\n",
      "        0.02500272, 0.01376319, 0.05258399, 0.07755184, 0.02630672],\n",
      "       [0.09391093, 0.04435523, 0.0700314 , 0.06814759, 0.01797609,\n",
      "        0.03571964, 0.07916583, 0.06649165, 0.02157638, 0.01330199],\n",
      "       [0.00247592, 0.03473391, 0.0562651 , 0.07283764, 0.02528474,\n",
      "        0.02914456, 0.09259367, 0.07123161, 0.0188848 , 0.00564077],\n",
      "       [0.08207645, 0.03333647, 0.09800736, 0.0643744 , 0.03627122,\n",
      "        0.05724639, 0.0162558 , 0.08623327, 0.02677456, 0.05208916],\n",
      "       [0.0271596 , 0.06811403, 0.02826544, 0.06259406, 0.08759722,\n",
      "        0.03244787, 0.06899722, 0.01670631, 0.01563199, 0.06739089],\n",
      "       [0.08344511, 0.09706727, 0.06488017, 0.05598654, 0.00628671,\n",
      "        0.05345313, 0.04173303, 0.03176998, 0.00614857, 0.02607285],\n",
      "       [0.00018098, 0.01236085, 0.01575763, 0.00461695, 0.08101127,\n",
      "        0.04939231, 0.01558868, 0.02596039, 0.04959181, 0.05875072],\n",
      "       [0.00079347, 0.08783584, 0.04800894, 0.04611664, 0.05447512,\n",
      "        0.0344274 , 0.0352676 , 0.00475698, 0.06650146, 0.00480946],\n",
      "       [0.0790964 , 0.03793795, 0.04930307, 0.06596154, 0.05306958,\n",
      "        0.00530126, 0.05476179, 0.01020528, 0.04914059, 0.07006981],\n",
      "       [0.01113041, 0.07500395, 0.08820817, 0.06126944, 0.03451787,\n",
      "        0.07668581, 0.09912534, 0.05677235, 0.03720548, 0.04920446],\n",
      "       [0.08906383, 0.03478042, 0.03729939, 0.04759813, 0.07826164,\n",
      "        0.09917558, 0.08399993, 0.08918981, 0.02358476, 0.00445999],\n",
      "       [0.02145321, 0.0113667 , 0.03779606, 0.0680025 , 0.05027031,\n",
      "        0.08130672, 0.07873741, 0.06767587, 0.04467079, 0.06841923],\n",
      "       [0.0370388 , 0.07569041, 0.082737  , 0.02875027, 0.09560663,\n",
      "        0.07456452, 0.02596786, 0.00517859, 0.09970868, 0.00071079],\n",
      "       [0.0698224 , 0.0081448 , 0.01602316, 0.07875245, 0.0549262 ,\n",
      "        0.04232257, 0.03852771, 0.06216073, 0.02928709, 0.08040977],\n",
      "       [0.0344064 , 0.03561575, 0.02505292, 0.04004954, 0.03072278,\n",
      "        0.08026317, 0.09571796, 0.09793604, 0.06500556, 0.05636346]])]\n",
      "[array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])]\n"
     ]
    }
   ],
   "source": [
    "#ip_neurons = 3 #input layer\n",
    "ip_neurons = 784\n",
    "num_hidden = 2\n",
    "encoding_bits=10\n",
    "sizes = [50,100]\n",
    "sizes.insert(0,ip_neurons)\n",
    "sizes.append(encoding_bits)\n",
    "print(sizes)\n",
    "print(len(sizes))\n",
    "\n",
    "weight_matrix_list = []\n",
    "\n",
    "bias_list =[]\n",
    "y_hat= []\n",
    "\n",
    "for i in range(int(num_hidden)+1):\n",
    "    weight_matrix_list.append(np.random.rand(sizes[i],sizes[i+1])/np.sqrt(sizes[i]))\n",
    "    bias_list.append(0*np.random.rand(sizes[i+1],1))\n",
    "print(weight_matrix_list)\n",
    "#print(len(weight_matrix_list))\n",
    "\n",
    "print(bias_list)\n",
    "#print(hlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "    \n",
    "    len_w = len(weight_matrix_list)\n",
    "    len_b = len(bias_list)\n",
    "    data_length = len(xtrain)\n",
    "    batch_size = 10\n",
    "    eta=0.01\n",
    "    no_of_batch = int(data_length/batch_size)\n",
    "    epochs =30\n",
    "    steps = 0\n",
    "    loss=0\n",
    "    error = 0\n",
    "    nos_points = 0\n",
    "    file = open('log_file.txt', 'w')\n",
    "    for t in range(epochs):\n",
    "        loss=0\n",
    "        error=0\n",
    "        for batch in range(no_of_batch):\n",
    "            dw=[]\n",
    "            db=[]\n",
    "            for i in range(int(num_hidden)+1):\n",
    "                dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "                db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "            for data_index_in_batch in range(batch_size):\n",
    "                l=batch_size*batch + data_index_in_batch\n",
    "                r=batch_size*batch + data_index_in_batch + 1\n",
    "#                 print(\"before\\n\",data_index_in_batch)\n",
    "                feedforward(xtrain[l:r,:],num_hidden)\n",
    "#                 print(\"after\\n\",y_hat)\n",
    "#                 print(l , r)\n",
    "                backpropagation(y_true[:,l:r])\n",
    "\n",
    "                for j in range(len_w):\n",
    "                    dw[j] += grad_w_k_list[j].transpose()\n",
    "\n",
    "                for j in range(len_b):\n",
    "                    db[j] += grad_b[j]\n",
    "                \n",
    "                \n",
    "                loss += cal_loss(y_hat,y_true[:,l:r])\n",
    "                \n",
    "#                 error += cal_error(y_hat,y_true[:,l:r])\n",
    "                \n",
    "#                 nos_points +=1\n",
    "                \n",
    "            for j in range(len_w):\n",
    "                weight_matrix_list[j] = weight_matrix_list[j] - eta*dw[j]\n",
    "\n",
    "            for j in range(len_b):\n",
    "                bias_list[j] =  bias_list[j] - eta*db[j]\n",
    "            \n",
    "            if (steps%300==0):\n",
    "                file.write(\"Epoch \"+str(t+1)+\", Step \"+str(steps)+\", loss : \"+str(loss)+\", error : \"+str(error/nos_points)+\", lr: \" + str(eta)+ \"\\n\")\n",
    "#                 print(\"loss \\n\",loss)\n",
    "            steps += 1\n",
    "            \n",
    "            \n",
    "#             print(\"after batch \", batch)\n",
    "#             print(\"y_hat\", y_hat)\n",
    "            \n",
    "#             print(\"subtracted weight\\n\", weight_matrix_list)\n",
    "            \n",
    "            \n",
    "            \n",
    "        print(\"end  epochs :  \", t, loss)\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(y_hat,y_true):\n",
    "    loss = 0\n",
    "    for i in range(10):\n",
    "        if(y_hat[i]!=0):\n",
    "            loss += (-1)*y_true[i]*math.log(y_hat[i]) \n",
    "    \n",
    "    return loss\n",
    "        \n",
    "def cal_error(y_hat,y_true):\n",
    "    true = classifcation(y_true)\n",
    "    predicted = classifcation(decode_yhat_to_classes(y_hat))\n",
    "    if (true != predicted):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end  epochs :   0 [82314.40653052]\n",
      "end  epochs :   1 [29755.57730271]\n",
      "end  epochs :   2 [20758.5163485]\n",
      "end  epochs :   3 [15594.96785681]\n",
      "end  epochs :   4 [11860.51197763]\n",
      "end  epochs :   5 [9117.23096232]\n",
      "end  epochs :   6 [6936.12526568]\n",
      "end  epochs :   7 [5459.28412708]\n",
      "end  epochs :   8 [4338.13116284]\n",
      "end  epochs :   9 [3671.32076224]\n",
      "end  epochs :   10 [3024.51611821]\n",
      "end  epochs :   11 [2444.70391476]\n",
      "end  epochs :   12 [1916.41267753]\n",
      "end  epochs :   13 [1512.8028853]\n",
      "end  epochs :   14 [1190.46374851]\n",
      "end  epochs :   15 [953.02689759]\n",
      "end  epochs :   16 [805.33976093]\n",
      "end  epochs :   17 [700.92753076]\n",
      "end  epochs :   18 [629.48692945]\n",
      "end  epochs :   19 [564.74146316]\n",
      "end  epochs :   20 [509.3282782]\n",
      "end  epochs :   21 [461.85418712]\n",
      "end  epochs :   22 [421.28061304]\n",
      "end  epochs :   23 [385.38913952]\n",
      "end  epochs :   24 [358.88395356]\n",
      "end  epochs :   25 [331.71697235]\n",
      "end  epochs :   26 [309.53481716]\n",
      "end  epochs :   27 [287.96988986]\n",
      "end  epochs :   28 [267.21883469]\n",
      "end  epochs :   29 [248.17235246]\n"
     ]
    }
   ],
   "source": [
    "grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes)\n",
    "#print(y_hat)0\n",
    "#print(len(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.64353364e-13 3.73949893e-16 4.19549592e-08 ... 9.59643030e-11\n",
      "  4.35510604e-16 8.83201612e-18]\n",
      " [4.08770298e-03 3.39618661e-16 9.99999038e-01 ... 1.62769773e-08\n",
      "  6.01440135e-11 1.18966762e-17]\n",
      " [8.05967789e-19 9.72224838e-09 2.03468069e-07 ... 1.54305413e-04\n",
      "  2.91701026e-08 6.46758407e-10]\n",
      " ...\n",
      " [6.97302944e-09 4.71779472e-09 3.63153787e-07 ... 1.95532211e-05\n",
      "  5.13815767e-18 5.86690192e-12]\n",
      " [1.94440597e-13 3.21956679e-10 2.03199742e-15 ... 1.03702674e-08\n",
      "  1.14170241e-16 5.59661210e-07]\n",
      " [5.13473003e-12 4.28922237e-06 3.34932279e-07 ... 4.71790691e-03\n",
      "  9.99998552e-01 1.07287603e-11]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feedforward(xtrain,num_hidden)\n",
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 1 ... 6 9 6]\n",
      "[[4]\n",
      " [6]\n",
      " [1]\n",
      " ...\n",
      " [6]\n",
      " [9]\n",
      " [6]]\n",
      "[[4.]\n",
      " [6.]\n",
      " [1.]\n",
      " ...\n",
      " [6.]\n",
      " [9.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)\n",
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993454545454545\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in range(55000):\n",
    "    if(y_predict[i]==true_label[i]):\n",
    "        cnt+=1\n",
    "acc = cnt/55000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "valid = pd.read_csv('valid.csv').as_matrix()\n",
    "xtrain=valid[:,1:785]\n",
    "\n",
    "xmean = np.mean(xtrain , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "xtrain = xtrain - means_expanded\n",
    "\n",
    "xstd = np.std(xtrain , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "xtrain = xtrain*1.0/std_expanded \n",
    "\n",
    "true_label = valid[:,785:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.10191211e-04 9.84131298e-06 2.83954028e-05 ... 8.20260749e-01\n",
      "  1.11356195e-02 1.68644264e-04]\n",
      " [9.59198810e-01 7.04160543e-04 3.83927059e-03 ... 1.76400583e-02\n",
      "  1.23599433e-02 5.09703830e-04]\n",
      " [8.98744705e-03 8.30065539e-03 5.05046872e-06 ... 3.03206052e-02\n",
      "  2.54696153e-04 9.94040979e-01]\n",
      " ...\n",
      " [2.80833030e-03 8.96574668e-04 4.74751884e-04 ... 1.08982260e-04\n",
      "  3.93649122e-01 3.69091508e-04]\n",
      " [5.01367429e-03 2.47946214e-04 6.24869263e-04 ... 8.18720513e-05\n",
      "  1.21391096e-01 4.50436386e-03]\n",
      " [1.75800697e-07 9.83015684e-01 1.69811286e-06 ... 2.78270759e-07\n",
      "  3.59006330e-07 1.26022428e-04]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feedforward(xtrain,num_hidden)\n",
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 9 4 ... 0 7 2]\n",
      "[[1]\n",
      " [9]\n",
      " [4]\n",
      " ...\n",
      " [0]\n",
      " [7]\n",
      " [2]]\n",
      "[[1.]\n",
      " [9.]\n",
      " [4.]\n",
      " ...\n",
      " [0.]\n",
      " [6.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)\n",
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))\n",
    "print(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7706\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "k = len(true_label)\n",
    "for i in range(k):\n",
    "    if(y_predict[i]==true_label[i]):\n",
    "        cnt+=1\n",
    "acc = cnt/k\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LN Pandey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "xtest = pd.read_csv('test.csv').as_matrix()\n",
    "test=xtest[:,1:]\n",
    "\n",
    "xmean = np.mean(test , axis=1)\n",
    "means_expanded = np.outer(xmean, np.ones(784))\n",
    "#print(means_expanded)\n",
    "test = test - means_expanded\n",
    "\n",
    "xstd = np.std(test , axis=1)\n",
    "std_expanded = np.outer(xstd, np.ones(784))\n",
    "#print(std_expanded)\n",
    "test = test*1.0/std_expanded \n",
    "\n",
    "feedforward(test,num_hidden)\n",
    "#print(y_hat)\n",
    "# y_hat = decode_yhat_to_classes(y_hat)\n",
    "#print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.71660489e-04 5.18170676e-02 4.55925136e-02 ... 1.19666898e-03\n",
      "  3.26600394e-04 1.85743674e-05]\n",
      " [2.15264564e-04 6.06552503e-04 3.92482707e-01 ... 4.56937506e-02\n",
      "  5.33260342e-05 9.96855782e-01]\n",
      " [1.02376299e-01 1.41460633e-01 6.32031913e-02 ... 8.54159541e-03\n",
      "  3.72155543e-03 1.66344453e-04]\n",
      " ...\n",
      " [4.03617587e-03 1.11531948e-01 2.94184984e-02 ... 3.01454251e-05\n",
      "  6.90161724e-02 2.02402223e-03]\n",
      " [1.00427913e-02 1.39394461e-01 7.13397059e-04 ... 5.54354918e-07\n",
      "  9.00542336e-01 8.15958935e-07]\n",
      " [4.39663052e-02 6.22723590e-05 3.45196609e-01 ... 9.30603883e-01\n",
      "  5.57629853e-05 1.71033985e-05]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)\n",
    "y_hat = decode_yhat_to_classes(y_hat)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifcation(y_hat):\n",
    "    y_predict=[]\n",
    "    col = len(y_hat[0])\n",
    "    for j in range(col):\n",
    "        for i in range(10):\n",
    "            if(y_hat[i][j]==1):\n",
    "                y_predict.append(i)\n",
    "                break\n",
    "    return np.array(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 1 ... 9 8 1]\n",
      "[[6]\n",
      " [6]\n",
      " [1]\n",
      " ...\n",
      " [9]\n",
      " [8]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y_predict = classifcation(y_hat)\n",
    "print((y_predict))\n",
    "y_predict = y_predict[:, np.newaxis]\n",
    "print((y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_to_y_predict(y_predict):\n",
    "    ide=[]\n",
    "    row=len(y_predict)\n",
    "    for i in range(row):\n",
    "        ide.append(i)\n",
    "    ide=np.array(ide)\n",
    "    ide=ide[:,np.newaxis]\n",
    "    y= np.hstack((ide,y_predict))\n",
    "    #print(y_predict, ide)\n",
    "    return y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [6]\n",
      " [1]\n",
      " ...\n",
      " [9]\n",
      " [8]\n",
      " [1]]\n",
      "[[   0    6]\n",
      " [   1    6]\n",
      " [   2    1]\n",
      " ...\n",
      " [9997    9]\n",
      " [9998    8]\n",
      " [9999    1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)\n",
    "#y_predict= np.random.randint(10, size=(10000, 1))\n",
    "y_predict= add_id_to_y_predict(y_predict)\n",
    "print(y_predict)\n",
    "x,y = y_predict.shape\n",
    "#print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predict_vanilla_grad_descent2.csv', 'w', newline='') as f:\n",
    "    thewriter = csv.writer(f)\n",
    "    row,col=y_predict.shape \n",
    "    thewriter.writerow(['id','label'])\n",
    "    for i in range(row):\n",
    "        thewriter.writerow(y_predict[i])\n",
    "#out.close()\n",
    "#np.savetxt(\"foo.csv\", y_predict, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.93764916e-15 4.05568165e-07 3.59781929e-14 ... 3.89901106e-10\n",
      "  7.59965544e-31 3.19362134e-15]\n",
      " [9.94611843e-01 2.68611797e-08 9.99999998e-01 ... 4.98472730e-10\n",
      "  7.70579926e-20 1.56223730e-04]\n",
      " [1.67894717e-12 6.30483232e-05 2.30425111e-09 ... 1.83559673e-08\n",
      "  8.51176117e-18 7.86255050e-09]\n",
      " ...\n",
      " [1.99193705e-09 3.19231017e-01 1.09848738e-15 ... 4.23982938e-08\n",
      "  9.55262076e-34 1.56278549e-04]\n",
      " [2.15413602e-17 7.96073113e-07 1.65730063e-20 ... 6.59005464e-18\n",
      "  2.67756736e-38 4.73634749e-04]\n",
      " [2.08866479e-21 1.35450757e-01 2.80597335e-19 ... 6.99727562e-05\n",
      "  1.00000000e+00 7.00380706e-04]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 1 ... 6 9 6]\n",
      "[[1]\n",
      " [5]\n",
      " [1]\n",
      " ...\n",
      " [6]\n",
      " [9]\n",
      " [6]]\n",
      "[[4.]\n",
      " [6.]\n",
      " [1.]\n",
      " ...\n",
      " [6.]\n",
      " [9.]\n",
      " [6.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7410363636363636\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in range(55000):\n",
    "    if(y_predict[i]==true_label[i]):\n",
    "        cnt+=1\n",
    "acc = cnt/55000\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "    \n",
    "    m_w = [] \n",
    "    m_b = []\n",
    "    for i in range(int(num_hidden)+1):\n",
    "        m_w.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "        m_b.append(0*np.random.rand(sizes[i+1],1))\n",
    "    \n",
    "    v_w = [] \n",
    "    v_b =[]\n",
    "    for i in range(int(num_hidden)+1):\n",
    "        v_w.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "        v_b.append(0*np.random.rand(sizes[i+1],1))\n",
    "        \n",
    "    len_w = len(weight_matrix_list)  \n",
    "    len_b = len(bias_list)\n",
    "    \n",
    "    eta = 0.0001\n",
    "    batch_size = 10\n",
    "    num_points_seen = 0\n",
    "    beta1 , beta2 , eps = 0.9 , 0.999 , 1e-8\n",
    "    epochs = 5\n",
    "    t=0\n",
    "    data_length = len(xtrain)\n",
    "    no_of_batch = int(data_length/batch_size)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        loss=0\n",
    "        for batch in range(no_of_batch):\n",
    "            dw=[]\n",
    "            db=[]\n",
    "            for i in range(int(num_hidden)+1):\n",
    "                dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "                db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                l = batch_size*batch + i\n",
    "                r = batch_size*batch + i + 1\n",
    "                feedforward(xtrain[l:r,:],num_hidden)\n",
    "                #print(l , r)\n",
    "                backpropagation(y_true[:,l:r])\n",
    "\n",
    "                for j in range(len_w): \n",
    "                    dw[j] += grad_w_k_list[j].transpose()\n",
    "                                    \n",
    "                for j in range(len_b):\n",
    "                    db[j] += grad_b[j]\n",
    "                \n",
    "                loss += cal_loss(y_hat,y_true[:,l:r])\n",
    "                                \n",
    "            for j in range(len_w): \n",
    "                m_w[j] = beta1*m_w[j] + (1-beta1)*dw[j]\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                m_b[j] = beta1*m_b[j] + (1-beta1)*db[j]    \n",
    "            \n",
    "            for j in range(len_w): \n",
    "                v_w[j] = beta2*v_w[j] + (1-beta2)*(dw[j]**2)\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                v_b[j] = beta2*v_b[j] + (1-beta2)*(db[j]**2)\n",
    "                \n",
    "            for j in range(len_w): \n",
    "                m_w[j] = (m_w[j]*1.0)/(1.0*(1-math.pow(beta1 , batch + 1)))\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                m_b[j] = (m_b[j]*1.0)/(1.0*(1-math.pow(beta1 , batch + 1)))\n",
    "             \n",
    "            for j in range(len_w): \n",
    "                v_w[j] = (v_w[j]*1.0)/(1.0*(1-math.pow(beta2 , batch + 1)))\n",
    "                                    \n",
    "            for j in range(len_b):\n",
    "                v_b[j] = (v_b[j]*1.0)/(1.0*(1-math.pow(beta2 , batch + 1)))\n",
    "                \n",
    "            for j in range(len_w):\n",
    "                weight_matrix_list[j] -= (eta/1.0*np.sqrt(v_w[j] + eps))*m_w[j]\n",
    "            \n",
    "            for j in range(len_b):\n",
    "                bias_list[j] -= (eta/1.0*np.sqrt(v_b[j] + eps))*m_b[j]\n",
    "        \n",
    "        print(\"end of epoch\" , t, loss)\n",
    "            \n",
    "#             print(\"batch\", batch)\n",
    "#             y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "#             print(y_hat)\n",
    "                \n",
    "#         print(\"end  epochs :  \", t)\n",
    "#         y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "#         print(y_hat)\n",
    "#     print(bias_list)\n",
    "#     print(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_func(weight_matrix_list,bias_list,test,hidden_layers):\n",
    "#     nos_data = len(test)\n",
    "#     #print(nos_data)\n",
    "#     #preactivation_list_new=[]\n",
    "#     #activation_list_new=[]\n",
    "#     #for i in range(nos_data):\n",
    "#     h = test.transpose()\n",
    "#     #preactivation_list_new.append(h)\n",
    "#     #activation_list_new.append(h)\n",
    "#     #print(h)\n",
    "#     for j in range(hidden_layers):\n",
    "#         a_new = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),h)\n",
    "#         #preactivation_list_new.append(a_new)\n",
    "#         h = 1.0/(1.0+np.exp(-a_new))\n",
    "#         #activation_list_new.append(h)\n",
    "#         #print(\"next activation\", h)\n",
    "\n",
    "#     j=hidden_layers\n",
    "#     #hlist.append(h)\n",
    "#     a_last = bias_list[j]+np.matmul(weight_matrix_list[j].transpose(),h)\n",
    "#     #preactivation_list.append(a_last)\n",
    "#     print(\"last activation\", a_last)\n",
    "#     #y_hat_new = np.array(output(a_last))\n",
    "#     #y_hat_new = y_hat_new.transpose()\n",
    "#     #print(\"y_hat\", y_hat)\n",
    "#     #print(\"y_hat_global\", y_hat_global)\n",
    "#     #print(y_hat.transpose())\n",
    "#     return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.array([\n",
    "#     [5,6,7],\n",
    "#     [6,7,8]\n",
    "# ]\n",
    "# )\n",
    "# y=test_func(weight_matrix_list,bias_list,test,num_hidden)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= np.array([\n",
    "#     [0,0,1,2,3],\n",
    "#     [1,1,2,3,4],\n",
    "#     [2,2,3,4,5],\n",
    "#     [3,3,4,5,6]\n",
    "# ]\n",
    "# )\n",
    "# y = np.array([\n",
    "#     [0,0,1,2,3],\n",
    "#     [1,1,2,3,4],\n",
    "#     [2,2,3,4,5],\n",
    "#     [3,3,4,5,6]\n",
    "# ]\n",
    "# )\n",
    "# print(np.subtract(x,y))\n",
    "# print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=np.tile(np.arange(1,10,10),(10,4))\n",
    "# print(s)\n",
    "# s[:,i:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s=np.tile(np.arange(1,10,10),(10,4))\n",
    "# print(s)\n",
    "# print(s[:,i:i+1])\n",
    "# p=softmax(s[:,i:i+1].transpose())\n",
    "# print(\"P\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array([\n",
    "#     [0,0,1,2,3],\n",
    "#     [1,1,2,3,4],\n",
    "#     [2,2,3,4,5],\n",
    "#     [3,3,4,5,6]\n",
    "# ])\n",
    "# #print(type(x))\n",
    "# y = np.mean(x , axis=1)\n",
    "# means_expanded = np.outer(y, np.ones(5))\n",
    "# print(means_expanded)\n",
    "# z=x-means_expanded\n",
    "# a = np.std(x,axis=1)\n",
    "# variance_extended = np.outer(a,np.ones(5))\n",
    "# print(a)\n",
    "# print(variance_extended)\n",
    "# unit_x= x/variance_extended\n",
    "# print(unit_x)\n",
    "# check = np.std(unit_x, axis=1)\n",
    "# print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = np.array([\n",
    "#     [0,0,1,2,7],\n",
    "#     [1,1,2,3,4],\n",
    "#     [2,2,3,4,5],\n",
    "#     [3,3,4,5,6]\n",
    "# ])\n",
    "# sum_col = np.sum(mat[0:1,:],axis=1)\n",
    "# print(sum_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output(a_last_layer,batch_train):\n",
    "#     y_hat_initial=[]\n",
    "#     y_hat_local=[]\n",
    "#     nos_data=len(batch_train)\n",
    "#     for i in range(nos_data):\n",
    "#         y_hat_initial.append(softmax(a_last_layer[:,i:i+1].transpose()))\n",
    "\n",
    "#     length = len(y_hat_initial)\n",
    "#     for i in range(length):\n",
    "#         y_hat_local.append(y_hat_initial[i][0])\n",
    "    \n",
    "#     print(\"y_hat_local\", y_hat_local)\n",
    "#     print(\"y_hat_initial\",y_hat_initial)\n",
    "#     return y_hat_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(row_vector):\n",
    "#     i = row_vector.size\n",
    "#     sum_all=0\n",
    "#     row_vector = np.exp(row_vector)\n",
    "#     for j in range(i):\n",
    "#         sum_all+= row_vector[0][j]\n",
    "#     row_vector = (row_vector*1.0/sum_all)\n",
    "#     return row_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad_descent(weight_matrix_list,bias_list,xtrain,num_hidden,sizes):\n",
    "#     t=0\n",
    "#     max_iteration=10\n",
    "    \n",
    "#     eta=0.01\n",
    "#     data_length = len(xtrain)\n",
    "#     batch_size = 10\n",
    "#     #batch_size = data_length\n",
    "#     no_of_batch = int(data_length/batch_size)\n",
    "#     #no_of_batch =1\n",
    "#     while(t < max_iteration):\n",
    "#         dw=[]\n",
    "#         db=[]\n",
    "        \n",
    "#         for i in range(int(num_hidden)+1):\n",
    "#             dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "#             db.append(0*np.random.rand(sizes[i+1],1))\n",
    "            \n",
    "        \n",
    "        \n",
    "#         for batch in range(no_of_batch):\n",
    "#             l=batch_size*batch\n",
    "#             r=batch_size*batch+batch_size\n",
    "#             feedforward(xtrain[l:r,:],num_hidden)\n",
    "#             backpropagation(y_true[:,l:r])\n",
    "            \n",
    "#             length=len(weight_matrix_list)\n",
    "#             i=0\n",
    "#             #print(\"grad_w\",grad_w_k_list)\n",
    "#             #print(\"a_matrix\",preactivation_list)\n",
    "#             #print(\"hactivation\",activation_list)\n",
    "#             #print(\"hat\",y_hat)\n",
    "            \n",
    "#             while(i<length):\n",
    "#                 dw[i] += grad_w_k_list[i].transpose()\n",
    "#                 i+=1\n",
    "#                 #print(\"weight_matrix_list\", weight_matrix_list[i])\n",
    "#                 #print(\"grad_w_k_list\", grad_w_k_list[i])\n",
    "#                 #print(\"weight_matrix_list\", weight_matrix_list[i])\n",
    "                \n",
    "#             length=len(bias_list)\n",
    "#             i=0\n",
    "            \n",
    "#             while(i<length):\n",
    "#                 db[i] += grad_b[i]\n",
    "#                 i+=1\n",
    "#                 #print(\"y_hat\", y_hat)\n",
    "#                 #y_hat_local = decode_yhat_to_classes(y_hat)\n",
    "#                 #if(batch == batch_size-1):\n",
    "#                 #print(\"y_hat\", y_hat)\n",
    "#                 #print(\"y_hat for batch \", batch , y_hat_local)\n",
    "        \n",
    "#         length=len(weight_matrix_list)\n",
    "#         i=0\n",
    "#         while(i<length):\n",
    "#             weight_matrix_list[i] -= eta*dw[i]\n",
    "#             i+=1\n",
    "        \n",
    "#         length=len(bias_list)\n",
    "#         i=0        \n",
    "#         while(i<length):\n",
    "#             bias_list[i] -= eta*db[i]\n",
    "#             i+=1\n",
    "#         t+=1\n",
    "#         print(\"end  epochs :  \", t)\n",
    "#         #print(y_hat)\n",
    "# #     print(bias_list)\n",
    "# #     print(preactivation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw=[]\n",
    "# db=[]\n",
    "        \n",
    "# for i in range(int(num_hidden)+1):\n",
    "#     dw.append(0*np.random.rand(sizes[i],sizes[i+1]))\n",
    "#     db.append(0*np.random.rand(sizes[i+1],1))\n",
    "# print(dw)\n",
    "# print(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_yhat_to_classes(y_hat):\n",
    "#     col = len(y_hat[0])\n",
    "#     max_val=0\n",
    "#     for j in range(col):\n",
    "#         max_val = y_hat[0][j]\n",
    "#         index=0\n",
    "#         for i in range(9):\n",
    "#             if(y_hat[i+1][j] > max_val):\n",
    "#                 y_hat[index][j]=0\n",
    "#                 index = i+1\n",
    "#                 max_val=y_hat[i+1][j]\n",
    "#             else :\n",
    "#                 y_hat[i+1][j]=0\n",
    "#         y_hat[index][j]=1\n",
    "#     return y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
